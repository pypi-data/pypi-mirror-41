{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute backends in AMLPF\n",
    "\n",
    "This notebook showcases the `compute` sub-package in AMLPF. The `compute` modules abstract several local or distributed backends\n",
    "available in Python such as [Joblib](https://joblib.readthedocs.io/en/latest/), [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) or [Azure Batch AI](https://docs.microsoft.com/en-us/azure/batch-ai/overview). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites: Setup and configure AML environment\n",
    "This notebook requires that a AML Python SDK is setup. Make sure you go through the [00. Installation and Configuration](https://github.com/Azure/ViennaDocs/blob/master/PrivatePreview/notebooks/00.configuration.ipynb) to do so if none is present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import FTK \n",
    "**NOTE**: If Pandas or other core library errors are encountered. Refresh the environment by reninstall the packages.\n",
    "This can be done by activating the kernel environment the notebook is being run under and then running the following command: `python.exe -m pip install -U --force-reinstall pandas==0.20.3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import os\n",
    "import urllib\n",
    "import pkg_resources\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import importlib\n",
    "from datetime import timedelta\n",
    "from random import randint\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import (TimeSeriesSplit, cross_val_score)\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "azureml_spec = importlib.util.find_spec(\"azureml.core\")\n",
    "if azureml_spec is not None:\n",
    "    import azureml.core\n",
    "    from azureml.core import Workspace, Run, Datastore\n",
    "    from azureml.core.runconfig import RunConfiguration       \n",
    "else:\n",
    "    print('AzureML not found')\n",
    "    raise\n",
    "\n",
    "from ftk import TimeSeriesDataFrame, ForecastDataFrame, AzureMLForecastPipeline\n",
    "from ftk.compute import ComputeBase, JoblibParallelCompute, DaskDistributedCompute, AMLBatchAICompute, Scheduler\n",
    "from ftk.data import load_dow_jones_dataset\n",
    "from ftk.transforms import LagLeadOperator, TimeSeriesImputer, TimeIndexFeaturizer, DropColumns\n",
    "from ftk.transforms.grain_index_featurizer import GrainIndexFeaturizer\n",
    "from ftk.models import Arima, SeasonalNaive, Naive, RegressionForecaster, BestOfForecaster\n",
    "from ftk.models.forecaster_union import ForecasterUnion\n",
    "from ftk.model_selection import TSGridSearchCV, RollingOriginValidator\n",
    "from ftk.ts_utils import last_n_periods_split\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "print(\"All imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset and engineer features\n",
    "\n",
    "Load the `Dominicks Orange Juice` dataset and perform feature engineering using available tranformers in AMLPF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the Dominicks OJ  dataset\n",
    "csv_path = pkg_resources.resource_filename('ftk', 'data/dominicks_oj/dominicks_oj.csv')\n",
    "whole_df = pd.read_csv(csv_path, low_memory = False)\n",
    "\n",
    "# Adjust 'Quantity` to be absolute value\n",
    "def expround(x):\n",
    "    return math.floor(math.exp(x) + 0.5)\n",
    "whole_df['Quantity'] = whole_df['logmove'].apply(expround)\n",
    "\n",
    "# Create new datetime columns containing the start and end of each week period\n",
    "weekZeroStart = pd.to_datetime('1989-09-07 00:00:00')\n",
    "weekZeroEnd = pd.to_datetime('1989-09-13 23:59:59')\n",
    "whole_df['WeekFirstDay'] = whole_df['week'].apply(lambda n: weekZeroStart + timedelta(weeks=n))\n",
    "whole_df['WeekLastDay'] = whole_df['week'].apply(lambda n: weekZeroEnd + timedelta(weeks=n))\n",
    "whole_df[['store','brand','WeekLastDay','Quantity']].head()\n",
    "\n",
    "# Create a TimeSeriesDataFrame\n",
    "# 'WeekLastDay' is the time index, 'Store' and 'brand'\n",
    "# combinations label the grain \n",
    "whole_tsdf = TimeSeriesDataFrame(whole_df, \n",
    "                                 grain_colnames=['store', 'brand'],\n",
    "                                 time_colname='WeekLastDay', \n",
    "                                 ts_value_colname='Quantity',\n",
    "                                 group_colnames='store')\n",
    "\n",
    "# sort and slice\n",
    "whole_tsdf.sort_index(inplace=True)\n",
    "\n",
    "# Get sales of dominick's brand orange juice from store 2 during summer 1990\n",
    "whole_tsdf.loc[pd.IndexSlice['1990-06':'1990-09', 2, 'dominicks'], ['Quantity']]\n",
    "\n",
    "train_tsdf, test_tsdf = last_n_periods_split(whole_tsdf, 40)\n",
    "\n",
    "# Use a TimeSeriesImputer to linearly interpolate missing values\n",
    "imputer = TimeSeriesImputer(input_column='Quantity', \n",
    "                            option='interpolate',\n",
    "                            method='linear',\n",
    "                            freq='W-WED')\n",
    "\n",
    "train_imputed_tsdf = imputer.transform(train_tsdf)\n",
    "\n",
    "# DropColumns: Drop columns that should not be included for modeling. `logmove` is the log of the number of \n",
    "# units sold, so providing this number would be cheating. `WeekFirstDay` would be \n",
    "# redundant since we already have a feature for the last day of the week.\n",
    "columns_to_drop = ['logmove', 'WeekFirstDay', 'week']\n",
    "column_dropper = DropColumns(columns_to_drop)\n",
    "\n",
    "# TimeSeriesImputer: Fill missing values in the features\n",
    "# First, we need to create a dictionary with key as column names and value as values used to fill missing \n",
    "# values for that column. We are going to use the mean to fill missing values for each column.\n",
    "columns_with_missing_values = train_imputed_tsdf.columns[pd.DataFrame(train_imputed_tsdf).isnull().any()].tolist()\n",
    "columns_with_missing_values = [c for c in columns_with_missing_values if c not in columns_to_drop]\n",
    "missing_value_imputation_dictionary = {}\n",
    "for c in columns_with_missing_values:\n",
    "    missing_value_imputation_dictionary[c] = train_imputed_tsdf[c].mean()\n",
    "fillna_imputer = TimeSeriesImputer(option='fillna', \n",
    "                                   input_column=columns_with_missing_values,\n",
    "                                   value=missing_value_imputation_dictionary)\n",
    "\n",
    "# TimeIndexFeaturizer: extract temporal features from timestamps\n",
    "time_index_featurizer = TimeIndexFeaturizer(correlation_cutoff=0.1, overwrite_columns=True)\n",
    "\n",
    "# GrainIndexFeaturizer: create indicator variables for stores and brands\n",
    "oj_series_freq = 'W-WED'\n",
    "oj_series_seasonality = 52\n",
    "grain_featurizer = GrainIndexFeaturizer(overwrite_columns=True, ts_frequency=oj_series_freq)\n",
    "\n",
    "pipeline_ml = AzureMLForecastPipeline([('drop_columns', column_dropper), \n",
    "                                       ('fillna_imputer', fillna_imputer),\n",
    "                                       ('time_index_featurizer', time_index_featurizer),\n",
    "                                       ('grain_featurizer', grain_featurizer)\n",
    "                                      ])\n",
    "\n",
    "\n",
    "train_feature_tsdf = pipeline_ml.fit_transform(train_imputed_tsdf)\n",
    "test_feature_tsdf = pipeline_ml.transform(test_tsdf)\n",
    "\n",
    "# Let's get a look at our new feature set\n",
    "print(train_feature_tsdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Rolling Origin Cross-Validation with a Random Forest model\n",
    "Perform a Rolling Origin cross validation to fit a model. In the sample below we use ROCV to fit a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the `RollingOriginValidator` to do 2 folds of rolling origin cross-validation\n",
    "roll_cv = RollingOriginValidator(n_splits=2)\n",
    "randomforest_model_for_cv = RegressionForecaster(estimator=RandomForestRegressor(),\n",
    "                                                 make_grain_features=False)\n",
    "\n",
    "# Set up our parameter grid and feed it to our grid search algorithm\n",
    "param_grid_rf = {'estimator__n_estimators': np.array([10, 100])}\n",
    "grid_cv_rf = TSGridSearchCV(randomforest_model_for_cv, param_grid_rf, cv=roll_cv)\n",
    "\n",
    "# fit and predict\n",
    "start = time.time()\n",
    "randomforest_cv_fitted= grid_cv_rf.fit(train_feature_tsdf, y=train_feature_tsdf.ts_value)\n",
    "print('Best parameter: {}'.format(randomforest_cv_fitted.best_params_))\n",
    "end = time.time()\n",
    "print('Total time taken to fit model:{}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model with FTK using `JoblibParallelCompute`\n",
    "\n",
    "Use the `JoblibParallelCompute` backend to parallelize the grid search and fit a model using ROCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_strategy_joblib = JoblibParallelCompute(job_count=16)\n",
    "grid_cv_rf.compute_strategy = compute_strategy_joblib\n",
    "\n",
    "start = time.time()\n",
    "# fit and predict\n",
    "randomforest_cv_fitted_joblib = grid_cv_rf.fit(train_feature_tsdf, y=train_feature_tsdf.ts_value)\n",
    "print('Best parameter: {}'.format(randomforest_cv_fitted_joblib.best_params_))\n",
    "end = time.time()\n",
    "print('Total time:{}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model with FTK using `DaskDistributedCompute`\n",
    "Use the `DaskDistributedCompute` backend to fit a model using ROCV. The default execution of this backend performs a process-based parallization of work such as the grid search in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_strategy_dask = DaskDistributedCompute()\n",
    "grid_cv_rf.compute_strategy = compute_strategy_dask\n",
    "\n",
    "start = time.time()\n",
    "# fit and predict\n",
    "randomforest_cv_fitted_dask = grid_cv_rf.fit(train_feature_tsdf, y=train_feature_tsdf.ts_value)\n",
    "print('Best parameter: {}'.format(randomforest_cv_fitted_dask.best_params_))\n",
    "end = time.time()\n",
    "print('Total time:{}'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a model using `AzureBatchAICompute`\n",
    "\n",
    "In the section below we show how [Azure Batch AI](https://docs.microsoft.com/en-us/azure/batch-ai/overview) can be used to distribute CV Search jobs to nodes in remote clusters leveraging the Azure Machine Learning's Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create or initialize an AML Workspace\n",
    "\n",
    "Initialize a workspace object from scratch or from persisted configuration. Note that you must have a valid Azure subscription for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create or fetch workspace\n",
    "\n",
    "# Provide valid Azure subscription id!\n",
    "subscription_id = \"00000000-0000-0000-0000-000000000000\"                    \n",
    "resource_group = \"amlpfbairg1\"\n",
    "workspace_name = \"workspace1\"\n",
    "workspace_region = \"eastus2\" # or eastus2euap\n",
    "\n",
    "ws = Workspace.create(name = workspace_name,\n",
    "                      subscription_id = subscription_id,\n",
    "                      resource_group = resource_group, \n",
    "                      location = workspace_region,\n",
    "                     exist_ok=True)\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Batch AI cluster as compute target\n",
    "Let's create a new Batch AI cluster in the current workspace, if it doesn't already exist. \n",
    "And use it to run the training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import BatchAiCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "# choose a name for your cluster\n",
    "batchai_cluster_name = 'amlpfbaicluster1'\n",
    "\n",
    "if batchai_cluster_name in ws.compute_targets():\n",
    "    compute_target = ws.compute_targets()[batchai_cluster_name]\n",
    "    if compute_target and type(compute_target) is BatchAiCompute:\n",
    "        print('Found compute target. Reusing: ' + batchai_cluster_name)\n",
    "else:\n",
    "    print('Creating new Batch AI compute target: ' + batchai_cluster_name)\n",
    "    provisioning_config = BatchAiCompute.provisioning_configuration(vm_size = vm_size, # NC6 is GPU-enabled\n",
    "                                                                vm_priority = 'lowpriority', # optional\n",
    "                                                                autoscale_enabled = autoscale_enabled,\n",
    "                                                                cluster_min_nodes = cluster_min_nodes, \n",
    "                                                                cluster_max_nodes = cluster_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())\n",
    "    print(compute_target.provisioning_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Run Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create run config\n",
    "runconfig = RunConfiguration()\n",
    "runconfig.target = batchai_cluster_name\n",
    "runconfig.batchai.node_count = 2\n",
    "runconfig.environment.docker.enabled = True\n",
    "\n",
    "# Set the datastore config in the runconfig\n",
    "_default_datastore = Datastore(ws)\n",
    "data_ref_configs = {}\n",
    "data_ref = _default_datastore._get_data_reference()\n",
    "data_ref_configs[data_ref.data_reference_name] = data_ref._to_config()\n",
    "runconfig.data_references = data_ref_configs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run an experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AMLBatchAI as the compute backend\n",
    "compute_strategy_batchai = AMLBatchAICompute(ws, runconfig)\n",
    "grid_cv_rf.compute_strategy = compute_strategy_batchai\n",
    "\n",
    "# Fit a model with CVSearch\n",
    "start = time.time()\n",
    "randomforest_cv_fitted_batchai = grid_cv_rf.fit(train_feature_tsdf, y=train_feature_tsdf.ts_value)\n",
    "end = time.time()\n",
    "\n",
    "# Results\n",
    "print('Best parameter: {}'.format(randomforest_cv_fitted_batchai.best_params_))\n",
    "print('Total time:{}'.format(end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azuremlftk_oct2018",
   "language": "python",
   "name": "azuremlftk_oct2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
