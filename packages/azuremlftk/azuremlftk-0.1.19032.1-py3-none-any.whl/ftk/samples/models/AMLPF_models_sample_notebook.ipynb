{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models in Azure Machine Learning Package for Forecasting\n",
    "This notebook demonstrates how to use the forecasting models available in Azure Machine Learning Package for Forecasting (AMLPF). The following types of models are covered:  \n",
    "\n",
    "* Univariate Time Series Models\n",
    "* Machine Learning Models\n",
    "* Model Union \n",
    "\n",
    "We will also briefly talk about model performance evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies for this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports done\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from ftk import TimeSeriesDataFrame\n",
    "from ftk.data import load_dominicks_oj_features\n",
    "from ftk.models import (Arima, SeasonalNaive, Naive, ExponentialSmoothing, \n",
    "                        RegressionForecaster, ForecasterUnion)\n",
    "\n",
    "print('imports done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Since the focus of this notebook is the AMLPF models, we load a preprocessed dataset with prepared features. Some features are from the [original dataset from Dominick's Finer Foods](https://research.chicagobooth.edu/kilts/marketing-databases/dominicks), and others are generated by the featurization transformers in AMLPF. Please see the sample notebooks on transformers for feature engieering tips with AMLPF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grain column names are ['store', 'brand']\n",
      "249 time series in the data frame.\n",
      "Group column names are ['store']\n",
      "83 stores/groups in the data frame.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>price</th>\n",
       "      <th>AGE60</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>ETHNIC</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>HHLARGE</th>\n",
       "      <th>WORKWOM</th>\n",
       "      <th>HVAL150</th>\n",
       "      <th>SSTRDIST</th>\n",
       "      <th>SSTRVOL</th>\n",
       "      <th>CPDIST5</th>\n",
       "      <th>CPWVOL5</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>grain_brand</th>\n",
       "      <th>grain_store</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeekLastDay</th>\n",
       "      <th>store</th>\n",
       "      <th>brand</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-06-20 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "      <td>10560.00</td>\n",
       "      <td>1990</td>\n",
       "      <td>20</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-06-27 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10133.33</td>\n",
       "      <td>1990</td>\n",
       "      <td>27</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-07-04 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.44</td>\n",
       "      <td>9706.67</td>\n",
       "      <td>1990</td>\n",
       "      <td>4</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-07-11 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.44</td>\n",
       "      <td>9280.00</td>\n",
       "      <td>1990</td>\n",
       "      <td>11</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-07-18 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>0.23</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.62</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.21</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.44</td>\n",
       "      <td>8853.33</td>\n",
       "      <td>1990</td>\n",
       "      <td>18</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     feat  price  AGE60  EDUC  ETHNIC  INCOME  \\\n",
       "WeekLastDay         store brand                                                 \n",
       "1990-06-20 23:59:00 2     dominicks  1.00   1.59   0.23  0.25    0.11   10.55   \n",
       "1990-06-27 23:59:00 2     dominicks  0.23   2.33   0.17  0.22    0.16   10.62   \n",
       "1990-07-04 23:59:00 2     dominicks  0.23   2.33   0.17  0.22    0.16   10.62   \n",
       "1990-07-11 23:59:00 2     dominicks  0.23   2.33   0.17  0.22    0.16   10.62   \n",
       "1990-07-18 23:59:00 2     dominicks  0.23   2.33   0.17  0.22    0.16   10.62   \n",
       "\n",
       "                                     HHLARGE  WORKWOM  HVAL150  SSTRDIST  \\\n",
       "WeekLastDay         store brand                                            \n",
       "1990-06-20 23:59:00 2     dominicks     0.10     0.30     0.46      2.11   \n",
       "1990-06-27 23:59:00 2     dominicks     0.12     0.36     0.34      5.10   \n",
       "1990-07-04 23:59:00 2     dominicks     0.12     0.36     0.34      5.10   \n",
       "1990-07-11 23:59:00 2     dominicks     0.12     0.36     0.34      5.10   \n",
       "1990-07-18 23:59:00 2     dominicks     0.12     0.36     0.34      5.10   \n",
       "\n",
       "                                     SSTRVOL  CPDIST5  CPWVOL5  Quantity  \\\n",
       "WeekLastDay         store brand                                            \n",
       "1990-06-20 23:59:00 2     dominicks     1.14     1.93     0.38  10560.00   \n",
       "1990-06-27 23:59:00 2     dominicks     1.21     2.12     0.44  10133.33   \n",
       "1990-07-04 23:59:00 2     dominicks     1.21     2.12     0.44   9706.67   \n",
       "1990-07-11 23:59:00 2     dominicks     1.21     2.12     0.44   9280.00   \n",
       "1990-07-18 23:59:00 2     dominicks     1.21     2.12     0.44   8853.33   \n",
       "\n",
       "                                     year  day grain_brand  grain_store  \n",
       "WeekLastDay         store brand                                          \n",
       "1990-06-20 23:59:00 2     dominicks  1990   20   dominicks            2  \n",
       "1990-06-27 23:59:00 2     dominicks  1990   27   dominicks            2  \n",
       "1990-07-04 23:59:00 2     dominicks  1990    4   dominicks            2  \n",
       "1990-07-11 23:59:00 2     dominicks  1990   11   dominicks            2  \n",
       "1990-07-18 23:59:00 2     dominicks  1990   18   dominicks            2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features_tsdf, test_features_tsdf = load_dominicks_oj_features()\n",
    "nseries = train_features_tsdf.groupby(train_features_tsdf.grain_colnames).ngroups\n",
    "nstores = len(train_features_tsdf.index.get_level_values(train_features_tsdf.group_colnames[0]).unique())\n",
    "print('Grain column names are {}'.format(train_features_tsdf.grain_colnames))\n",
    "print('{} time series in the data frame.'.format(nseries))\n",
    "print('Group column names are {}'.format(train_features_tsdf.group_colnames))\n",
    "print('{} stores/groups in the data frame.'.format(nstores))\n",
    "train_features_tsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data contains 249 different combinations of store and brand in a data frame. Each combination defines its own time series of sales. \n",
    "\n",
    "The difference between _grain_ and _group_ is that _grain_ usually identifies a single time series in the raw data (without multi-horizon features), while _group_ can contain multiple time series in the raw data. As will be shown later, internal package functions use group to build a single model from multiple time series if the user believes this grouping helps improve model performance. By default, group is set to be equal to grain, and a single model is built for each grain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Time Series Models\n",
    "\n",
    "A univariate time series is a sequence of observations of the same variable recorded over time, ususally at regular time intervals. Univaraite time series models analyze the temporal patterns, e.g. trend, seasonality, in the target variable to forecast future values of the target variable.  \n",
    "The following univariate models are available in AMLPF. \n",
    "\n",
    "* The **Naive** forecasting algorithm uses the actual target variable value of the last period as the forecasted value of the current period.\n",
    "\n",
    "* The **Seasonal Naive** algorithm uses the actual target variable value of the same time point of the previous season as the forecasted value of the current time point. Some examples include using the actual value of the same month of last year to forecast months of the current year; use the same hour of yesterday to forecast hours today. \n",
    "\n",
    "* The **Exponential Smoothing (ES)** algorithm generates forecasts by computing the weighted averages of past observations, with the weights decaying exponentially as the observations get older. \n",
    "\n",
    "* The **AutoRegressive Integrated Moving Average (ARIMA)** algorithm captures the autocorrelation in time series data. For more information about ARIMA, see [this link](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the univariate models only utilizes the sales values over time, we extract the sales values column to save computation time and space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Quantity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeekLastDay</th>\n",
       "      <th>store</th>\n",
       "      <th>brand</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-06-20 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>10560.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-06-27 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>10133.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-07-04 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>9706.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-07-11 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>9280.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-07-18 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <td>8853.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Quantity\n",
       "WeekLastDay         store brand              \n",
       "1990-06-20 23:59:00 2     dominicks  10560.00\n",
       "1990-06-27 23:59:00 2     dominicks  10133.33\n",
       "1990-07-04 23:59:00 2     dominicks   9706.67\n",
       "1990-07-11 23:59:00 2     dominicks   9280.00\n",
       "1990-07-18 23:59:00 2     dominicks   8853.33"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tsdf =  TimeSeriesDataFrame(train_features_tsdf[train_features_tsdf.ts_value_colname],\n",
    "                                  grain_colnames=['store', 'brand'],\n",
    "                                  time_colname='WeekLastDay',\n",
    "                                  ts_value_colname='Quantity',\n",
    "                                  group_colnames='store')\n",
    "test_tsdf =  TimeSeriesDataFrame(test_features_tsdf[test_features_tsdf.ts_value_colname],\n",
    "                                 grain_colnames=['store', 'brand'],\n",
    "                                 time_colname='WeekLastDay',\n",
    "                                 ts_value_colname='Quantity',\n",
    "                                 group_colnames='store')\n",
    "train_tsdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the frequency and seasonality parameters for univariate models.   \n",
    "**Frequency** is the time interval at which the observations are recorded, e.g. daily, weekly, monthly. The frequency of the Dominick's data is weekly, ended on every Wednesday. The frequency of a dataset can be obtained by calling the `get_frequency_dict` method of a TimeSeriesDataFrame.  \n",
    "**Seasonality** is a periodic pattern in time series data with a fixed and known period. This pattern is usually associated with some aspect of the calendar. For example, a time series with quarterly frequency presents repeated pattern every four quarters, then the seasonality of this time series is 4. The Dominick's data don't present any strong seasonality pattern. Here we assume a yearly seasonality, which is 52 (weeks). The seasonality of a dataset can be obtained by calling the `get_seasonality_dict` of a TimeSeriesDataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_freq = 'W-WED'\n",
    "series_seasonality = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Univariate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model = Naive(freq=series_freq)\n",
    "\n",
    "seasonal_naive_model = SeasonalNaive(freq=series_freq, \n",
    "                                     seasonality=series_seasonality)\n",
    "\n",
    "# Initialize Exponential Smoothing model\n",
    "# Don't allow multiplicative trend since it can lead to instability\n",
    "es_model = ExponentialSmoothing(freq=series_freq,\n",
    "                                allow_multiplicative_trend=False)\n",
    "\n",
    "arima_order = [2, 1, 0]\n",
    "arima_model = Arima(series_freq, arima_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Univariate Models\n",
    "The estimators in AMLPF follow the same API as scikit-learn estimators: a `fit` method for model training and a `predict` method for generating forecasts.  \n",
    "Since these models are all univariate models, one model is fit on each grain of the data. Using AMLPF, all 249 models can be fit with just one function call.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_model_fitted = naive_model.fit(train_tsdf)\n",
    "seasonal_naive_model_fitted = seasonal_naive_model.fit(train_tsdf)\n",
    "es_model_fitted = es_model.fit(train_tsdf)\n",
    "arima_model_fitted = arima_model.fit(train_tsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast/Predict with Univariate Models\n",
    "Once the models are trained, you can generate forecast by calling the `predict` method with the testing/scoring/new data. Similar to the fit method, you can create predictions for all 249 series in the testing dataset with one call to the `predict` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Quantity</th>\n",
       "      <th>DistributionForecast</th>\n",
       "      <th>PointForecast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeekLastDay</th>\n",
       "      <th>store</th>\n",
       "      <th>brand</th>\n",
       "      <th>origin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1992-01-08 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>11712.00</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>10217.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-15 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>4032.00</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>8814.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-22 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>6336.00</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>9809.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-29 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>13632.00</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>9648.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-02-05 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>45120.00</td>\n",
       "      <td>&lt;scipy.stats._distn_infrastructure.rv_frozen o...</td>\n",
       "      <td>9399.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         Quantity  \\\n",
       "WeekLastDay         store brand     origin                          \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00  11712.00   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00   4032.00   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00   6336.00   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00  13632.00   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00  45120.00   \n",
       "\n",
       "                                                                                      DistributionForecast  \\\n",
       "WeekLastDay         store brand     origin                                                                   \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00  <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00  <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00  <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00  <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00  <scipy.stats._distn_infrastructure.rv_frozen o...   \n",
       "\n",
       "                                                         PointForecast  \n",
       "WeekLastDay         store brand     origin                              \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00       10217.31  \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00        8814.64  \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00        9809.96  \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00        9648.68  \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00        9399.18  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_model_forecast = naive_model_fitted.predict(test_tsdf)\n",
    "seasonal_naive_model_forecast = seasonal_naive_model_fitted.predict(test_tsdf)\n",
    "es_model_forecast = es_model_fitted.predict(test_tsdf)\n",
    "arima_model_forecast = arima_model_fitted.predict(test_tsdf)\n",
    "arima_model_forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `predict` method is a [ForecastDataFrame](https://docs.microsoft.com/en-us/python/api/ftk.dataframe_forecast.forecastdataframe?view=azure-ml-py-latest) with point and distribution forecast columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "\n",
    "In addition to traditional univariate models, Azure Machine Learning Package for Forecasting also enables you to create machine learning models for forecasting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegressionForecaster\n",
    "\n",
    "The [RegressionForecaster](https://docs.microsoft.com/en-us/python/api/ftk.models.regression_forecaster.regressionforecaster?view=azure-ml-py-latest)  function wraps scikit-learn regression estimators so that they can be trained on [TimeSeriesDataFrame](https://docs.microsoft.com/en-us/python/api/ftk.dataframe_ts.timeseriesdataframe?view=azure-ml-py-latest). The wrapped forecasters have the following functionalities:\n",
    "1. Put each `group` of data into the same model, so that it can learn one model for a group of series that are deemed similar and can be pooled together. One model for a group of series often uses the data from longer series to improve forecasts for short series. \n",
    "2. Create one-hot encoding for categorical features, if `internal_featurization` is set to `True`, because scikit-learn estimators can generally only accept numeric features.\n",
    "3. Create `grain` and `horizon` features, if both `internal_featurization` and `make_grain_features` are set to `True`.   \n",
    "\n",
    "Here we demonstrate a couple of regression models. You can substitute these models for any other models in sckit-learn that support regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"make_grain_features\" to False, because our data already contain grain and horizon features\n",
    "random_forest_model = RegressionForecaster(estimator=RandomForestRegressor(),\n",
    "                                           make_grain_features=False)\n",
    "boosted_trees_model = RegressionForecaster(estimator=GradientBoostingRegressor(),\n",
    "                                           make_grain_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random_forest_model_fitted = random_forest_model.fit(train_features_tsdf)\n",
    "boosted_trees_model_fitted = boosted_trees_model.fit(train_features_tsdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecast/Predict with Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>feat</th>\n",
       "      <th>price</th>\n",
       "      <th>AGE60</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>ETHNIC</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>HHLARGE</th>\n",
       "      <th>WORKWOM</th>\n",
       "      <th>HVAL150</th>\n",
       "      <th>SSTRDIST</th>\n",
       "      <th>SSTRVOL</th>\n",
       "      <th>CPDIST5</th>\n",
       "      <th>CPWVOL5</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>year</th>\n",
       "      <th>day</th>\n",
       "      <th>grain_brand</th>\n",
       "      <th>grain_store</th>\n",
       "      <th>DistributionForecast</th>\n",
       "      <th>PointForecast</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WeekLastDay</th>\n",
       "      <th>store</th>\n",
       "      <th>brand</th>\n",
       "      <th>origin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1992-01-08 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "      <td>11712.00</td>\n",
       "      <td>1992</td>\n",
       "      <td>8</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>6174.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-15 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "      <td>4032.00</td>\n",
       "      <td>1992</td>\n",
       "      <td>15</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>1354.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-22 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.82</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "      <td>6336.00</td>\n",
       "      <td>1992</td>\n",
       "      <td>22</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>2299.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-29 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "      <td>13632.00</td>\n",
       "      <td>1992</td>\n",
       "      <td>29</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>-4761.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-02-05 23:59:00</th>\n",
       "      <th>2</th>\n",
       "      <th>dominicks</th>\n",
       "      <th>1992-01-01 23:59:00</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.11</td>\n",
       "      <td>10.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.38</td>\n",
       "      <td>45120.00</td>\n",
       "      <td>1992</td>\n",
       "      <td>5</td>\n",
       "      <td>dominicks</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>8389.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         feat  price  AGE60  \\\n",
       "WeekLastDay         store brand     origin                                    \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00  0.00   1.69   0.23   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00  0.00   1.76   0.23   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00  0.00   1.82   0.23   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00  0.00   1.47   0.23   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00  0.00   1.29   0.23   \n",
       "\n",
       "                                                         EDUC  ETHNIC  INCOME  \\\n",
       "WeekLastDay         store brand     origin                                      \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00  0.25    0.11   10.55   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00  0.25    0.11   10.55   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00  0.25    0.11   10.55   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00  0.25    0.11   10.55   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00  0.25    0.11   10.55   \n",
       "\n",
       "                                                         HHLARGE  WORKWOM  \\\n",
       "WeekLastDay         store brand     origin                                  \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00     0.10     0.30   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00     0.10     0.30   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00     0.10     0.30   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00     0.10     0.30   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00     0.10     0.30   \n",
       "\n",
       "                                                         HVAL150  SSTRDIST  \\\n",
       "WeekLastDay         store brand     origin                                   \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00     0.46      2.11   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00     0.46      2.11   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00     0.46      2.11   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00     0.46      2.11   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00     0.46      2.11   \n",
       "\n",
       "                                                         SSTRVOL  CPDIST5  \\\n",
       "WeekLastDay         store brand     origin                                  \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00     1.14     1.93   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00     1.14     1.93   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00     1.14     1.93   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00     1.14     1.93   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00     1.14     1.93   \n",
       "\n",
       "                                                         CPWVOL5  Quantity  \\\n",
       "WeekLastDay         store brand     origin                                   \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00     0.38  11712.00   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00     0.38   4032.00   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00     0.38   6336.00   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00     0.38  13632.00   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00     0.38  45120.00   \n",
       "\n",
       "                                                         year  day  \\\n",
       "WeekLastDay         store brand     origin                           \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00  1992    8   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00  1992   15   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00  1992   22   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00  1992   29   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00  1992    5   \n",
       "\n",
       "                                                        grain_brand  \\\n",
       "WeekLastDay         store brand     origin                            \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00   dominicks   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00   dominicks   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00   dominicks   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00   dominicks   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00   dominicks   \n",
       "\n",
       "                                                         grain_store  \\\n",
       "WeekLastDay         store brand     origin                             \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00            2   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00            2   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00            2   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00            2   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00            2   \n",
       "\n",
       "                                                        DistributionForecast  \\\n",
       "WeekLastDay         store brand     origin                                     \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00                 None   \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00                 None   \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00                 None   \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00                 None   \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00                 None   \n",
       "\n",
       "                                                         PointForecast  \n",
       "WeekLastDay         store brand     origin                              \n",
       "1992-01-08 23:59:00 2     dominicks 1992-01-01 23:59:00        6174.86  \n",
       "1992-01-15 23:59:00 2     dominicks 1992-01-01 23:59:00        1354.05  \n",
       "1992-01-22 23:59:00 2     dominicks 1992-01-01 23:59:00        2299.09  \n",
       "1992-01-29 23:59:00 2     dominicks 1992-01-01 23:59:00       -4761.23  \n",
       "1992-02-05 23:59:00 2     dominicks 1992-01-01 23:59:00        8389.55  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random_forest_forecast = random_forest_model_fitted.predict(test_features_tsdf)\n",
    "boosted_trees_forecast = boosted_trees_model_fitted.predict(test_features_tsdf)\n",
    "boosted_trees_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ftk.forecast_data_frame.ForecastDataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(boosted_trees_forecast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"actual\": \"Quantity\", \"datetime_colnames_all\": [\"WeekLastDay\", \"origin\"], \"grain_colnames\": [\"store\", \"brand\"], \"time_colname\": \"WeekLastDay\", \"pred_point\": \"PointForecast\", \"origin_time_colname\": \"origin\", \"data\": \"{\\\\\"columns\\\\\":[\\\\\"WeekLastDay\\\\\",\\\\\"store\\\\\",\\\\\"brand\\\\\",\\\\\"origin\\\\\",\\\\\"Quantity\\\\\",\\\\\"PointForecast\\\\\"],\\\\\"index\\\\\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_qty = 'Quantity'\n",
    "_store = 'store'\n",
    "_brand = 'brand'\n",
    "_date = 'WeekLastDay'\n",
    "cols = [_qty, 'PointForecast']\n",
    "boosted_trees_forecast_out = boosted_trees_forecast[cols]\n",
    "boosted_trees_forecast_out.to_json()[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Multiple Models\n",
    "\n",
    "The [ForecasterUnion](https://docs.microsoft.com/en-us/python/api/ftk.models.forecaster_union.forecasterunion?view=azure-ml-py-latest) estimator allows you to combine multiple estimators and fit/predict on them using one line of code. Here we combine all the models created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster_union = ForecasterUnion(\n",
    "    forecaster_list=[('naive', naive_model), \n",
    "                     ('seasonal_naive', seasonal_naive_model), \n",
    "                     ('es', es_model), \n",
    "                     ('arima', arima_model),\n",
    "                     ('random_forest', random_forest_model),\n",
    "                     ('boosted_trees', boosted_trees_model)])\n",
    "forecaster_union_fitted = forecaster_union.fit(train_features_tsdf)\n",
    "forecaster_union_forecast = forecaster_union_fitted.predict(test_features_tsdf, retain_feature_column=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "Now you can calculate the forecast errors on the test set. You can use the mean absolute percentage error (MAPE) here. MAPE is the mean absolute percent error relative to the actual sales values. The ```calc_error``` function provides a few built-in functions for commonly used error metrics. You can also define your custom error function to calculate other metrics, e.g. MedianAPE, and pass it to the `err_fun` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median_ape(y_true, y_pred):\n",
    "    y_true = np.array(y_true).astype(float)\n",
    "    y_pred = np.array(y_pred).astype(float)\n",
    "    y_true_rm_na = y_true[~(np.isnan(y_true) | np.isnan(y_pred))]\n",
    "    y_pred_rm_na = y_pred[~(np.isnan(y_true) | np.isnan(y_pred))]\n",
    "    y_true = y_true_rm_na\n",
    "    y_pred = y_pred_rm_na\n",
    "    if len(y_true) == 0:\n",
    "        # if there is no entries left after removing na data, return np.nan\n",
    "        return(np.nan)\n",
    "    y_true_rm_zero = y_true[y_true != 0]\n",
    "    y_pred_rm_zero = y_pred[y_true != 0]\n",
    "    if len(y_true_rm_zero) == 0:\n",
    "        # if all values are zero, np.nan will be returned.\n",
    "        return(np.nan)\n",
    "    ape = np.abs((y_true_rm_zero - y_pred_rm_zero) / y_true_rm_zero) * 100\n",
    "    median_ape = np.median(ape)\n",
    "    return median_ape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModelName</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>MedianAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>71.68</td>\n",
       "      <td>43.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boosted_trees</td>\n",
       "      <td>74.75</td>\n",
       "      <td>47.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>seasonal_naive</td>\n",
       "      <td>182.93</td>\n",
       "      <td>66.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arima</td>\n",
       "      <td>106.39</td>\n",
       "      <td>67.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naive</td>\n",
       "      <td>134.75</td>\n",
       "      <td>73.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es</td>\n",
       "      <td>144.19</td>\n",
       "      <td>77.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ModelName   MAPE  MedianAPE\n",
       "4   random_forest  71.68      43.23\n",
       "1   boosted_trees  74.75      47.33\n",
       "5  seasonal_naive 182.93      66.19\n",
       "0           arima 106.39      67.46\n",
       "3           naive 134.75      73.37\n",
       "2              es 144.19      77.43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecaster_union_MAPE = forecaster_union_forecast.calc_error(err_name='MAPE',\n",
    "                                                             by='ModelName')\n",
    "forecaster_union_MedianAPE = forecaster_union_forecast.calc_error(err_name='MedianAPE', \n",
    "                                                                  err_fun=calc_median_ape,\n",
    "                                                                  by='ModelName')\n",
    "all_model_errors = forecaster_union_MAPE.merge(forecaster_union_MedianAPE, on='ModelName')\n",
    "all_model_errors.sort_values('MedianAPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning models are able to take advantage of the added features and the similarities between series to get better forecast accuracy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
