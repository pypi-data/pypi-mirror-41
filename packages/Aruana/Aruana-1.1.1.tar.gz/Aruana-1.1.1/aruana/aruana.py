# imports
import emoji
import math
import numpy as np
import os
import pandas as pd
import pickle
import random
import re
import time
import unicodedata
import urllib.request
import warnings
import zipfile
from aruana import same_words
from aruana import stopwords
from aruana import strings
from aruana import vocab
from collections import Counter
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation
from keras.optimizers import Adam
from keras import backend as K
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
from matplotlib import pyplot as plt
from nltk.stem.snowball import SnowballStemmer
from sklearn.model_selection import train_test_split
from tqdm import tqdm

class Aruana:
    """Aruana is a collection of methods that can be used for simple NLP tasks.
    It can be used for tasks involving text preprocessing for machine learning.
    Aruana works mainly with text strings and lists of strings. 

    It requires NLTK and TQDM to be installed.

        Attributes
        ----------
        language : str
            The language used to initialize Aruana. Accepts 'pt-br', 'en' and 'fr'
        nltk_language : str
            The language used for NLTK, generated by the method __language_for_nltk
        vocab : Counter()
            The number of unique tokens found on the text
        path : str
            Used to tell where to look for the models used by Aruana. Default is Aruana workspace
        model_type : str
            The name of the model that has to be loaded. Default is the tagger model
        maxlen : int
            The value used for sentence padding. Is updated automatically after a model is loaded
        model
            the model that has to be loaded by Aruana

        Methods
        -------
        __language_for_nltk()
            Converts the language inputed during Aruana init to a format that can be used by NLTK
        
        remove_html_tags(text:str)
            Removes html tags from text

        replace_html_tags(text:str, placeholder:str)
            Replaces html tags from text by a given placeholder
        
        remove_urls(text:str)
            Removes urls from text
        
        replace_urls(text:str, placeholder:str)
            Replaces links from text by a given placeholder
        
        remove_hashtags(text:str)
            Removes hashtags from text
        
        replace_hashtags(text:str, placeholder:str)
            Replaces hashtags from text by a given placeholder
        
        remove_ips(text:str)
            Removes ips from text

        replace_ips(text:str, placeholder:str)
            Replaces ips from text by a given placeholder

        remove_handles(text:str)
            Removes handles from text

        replace_handles(text:str, placeholder:str)
            Replaces handles from text by a given placeholder

        strip_accents(text:str)
            Strip accents from tokens
        
        replace_quotes(text:str, placeholder:str)
            Replaces quotes by a placeholder

        remove_punctuation(text:str)
            Removes ponctuation and special chars
        
        replace_punctuation(text:str, sign='', placeholder='')
            Replaces punctuation by a placeholder
        
        remove_numbers(text:str)
            Removes all numbers

        replace_numbers(text:str)
            Replaces all numbers for string

        stem_sentence(text:str)
            Uses NLTK stemmer to stem tokens

        remove_stopwords(text:str, custom_list=[], extend_set=False)
            Removes stopwords

        reduce_words_with_repeated_chars(text:str)
            Reduces words with chars repeated more than 3 times to a single char. 
            Useful to replace words such as loooooooong by long. Be careful, 
            as it can change abreviations such as AAA to single A 
        
        remove_excessive_spaces(text:str)
            Removes excessive space from text

        merge_same_tokens(text:str)
            If two tokens are the same, this will merge them. 
            Specially useful when dealing with social media text.

        expand_contractions(text:str)
            Expands some of the most popular English contractions

        lower_remove_white(text:str)
            Use it to lower text and to remove training whitespaces

        tokenize(text:str):
            Receives a string and splits it into tokens.
            Returns a list with all the tokens found.

        preprocess(text:str, tokenize=True, stem=True, remove_stopwords=False, tag_text=False)
            Preprocess text before data handling with most common settings.
            Text is processed in the following order:

            - lower text
            - strip trailing whitespaces
            - convert emojis to text
            - expand contractions
            - replace urls
            - remove tags
            - replace hashtags
            - replace ips
            - remove social media handles
            - tags text if True
            - remove stopwords if True
            - remove accents
            - remove punctuation        
            - replace numbers
            - reduce loooooong words
            - remove excessive spaces
            - stem text if True
            - tokenize text (if True, will return a list of tokens)
        
        preprocess_list(text_list:list, tokenize=True, stem=True, remove_stopwords=False, tag_text=False)
            Preprocess every text in a list of strings

        create_corpus(text_list:list)
            Receives a list of strings and returns a single string with all the text

        vocab_size(corpus:str)
            Returns the number of tokens found on the corpus

        lexical_diversity(corpus_slice:str, corpus:str)
            Counts the vocab of one part of the corpus and compares it to the entire corpus

        random_classification(list_to_classify:list, classes:list, balanced=True):
        
            Receives a list of sentences and assigns randomly a label for each item 
            on the list. Returns a list with all the random labels.

        replace_with_blob(list_of_texts:list):
        
            Receives a list of sentences and creates blobs with the same 
            vocabulary found on corpus.

        tag_tokens(text:str, sep="_"):
        
            Tags each token on a list of tokens and merges their tags to the original token
        
        ingest(path, text_column = ''):
        
            Reads a csv file and returns the corpora in the form of a list of sentences.
        
        _save_object(object_name:str, object_to_save):

            Receives an Python object and saves it to Aruana work space using pickle

        _path_model_language(path='', model_name='', language=''):
            
            Defines the path, the model and the language to save or load a model
        
        _load_models():
        
            Loads a previously saved model
            
        _load_object(object_name:str)
        
            Loads a Python object saved on the Aruana work space on pickle format
        
        train_annotations(annotated_sentences:list, 
                          path='',
                          model_name='',
                          language='',
                          embedding_output_dim=128,
                          lstm_units=256,
                          save_model=True, 
                          batch_size=128,
                          epochs=40,
                          validation_split=0.2):

            Trains a new annotation model on a set of annotated tokens. A new model
            will be created and saved on Aruana workspace, erasing any previous model
            saved with the same name. The local where you want to save your model, the 
            name and the language can be changed.
        
        _to_categorical(sequences:list, categories:int)
        
            Transforms the sequences of tags to sequences of categorical values
        
        _word_to_int_dict(train_sentences:list, 
                         test_sentences:list, 
                         train_tags:list, 
                         test_tags:list):

            Creates a dictionary with the tokens and assign an int to each token
        
        _logits_to_tokens(sequences:list, index:int):

            Converts the integers back to the tags
        
        annotate(list_to_predict:list):
        
            Does the annotations on a new unseen list of sentences using a previously saved model. 

        download(model_name='', model_version='', language=''):
        
            Downloads and saves the models necessary for Annotation execution
        
    """

    def __init__(self, language:str, model_type="tagger", model_version="0_0_1", path='/usr/local/share/aruana_data/models/'):
        """
        Parameters
        ----------
        language : str
            The language used to initialize Aruana. Accepts 'pt-br', 'en' and 'fr'
        nltk_language : str
            The language used for NLTK, generated by the method language_for_nltk
        vocab : Counter()
            The number of unique tokens found on the text
        path : str
            Used to tell where to look for the models used by Aruana. Default is Aruana workspace
        model_type : str
            The name of the model that has to be loaded. Default is the tagger model
        maxlen : int
            The value used for sentence padding. Is updated automatically after a model is loaded
        model
            the model that has to be loaded by Aruana
        """
        self.language = language
        self.nltk_language = self.__language_for_nltk()
        self.vocab = Counter()
        self.path = path + model_type + '/'
        self.model_type = model_type
        self.model_version = model_version
        self.maxlen = 0
        self.pos_tagger_model = self._load_models(model_name='tagger')

    def __language_for_nltk(self):
        """ Converts the language inputed during Aruana init to a format that can be used by NLTK"""
        if self.language == 'pt-br':
            return 'portuguese'
        if self.language == 'en':
            return 'english'
        if self.language == 'fr':
            return 'french'
        elif self.language == 'custom':
            return 'portuguese'

    def remove_html_tags(self, text:str):
        """Removes html tags from text

        >>> remove_html_tags('<h1><strong>Hello world!</strong></h1>')
        'Hello world!'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub(r'<[^>]*>', '', text)
        return text

    def replace_html_tags(self, text:str, placeholder:str):
        """Replaces html tags from text by a given placeholder
        
        >>> replace_html_tags('<h1><strong>Hello world!</strong></h1>', 'HTML')
        'HTML HTML Hello world! HTML HTML'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the html tag
        """
        text = re.sub(r'<[^>]*>', ' ' + placeholder + ' ', text)
        text = self.remove_excessive_spaces(text)
        return text

    def remove_urls(self, text:str):
        """Removes urls from text

        >>> remove_urls('You can go to the page http://homepage.com to see the content.')
        'You can go to the page to see the content.'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_urls(self, text:str, placeholder:str):
        """Replaces links from text by a given placeholder
        
        >>> replace_urls('You can go to the page http://homepage.com to see the content.', 'URL')
        'You can go to the page URL to see the content.'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the url
        """
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', placeholder, text)
        return text

    def remove_hashtags(self, text:str):
        """Removes hashtags from text
        
        >>> remove_hashtags('I wish you all #love and #peanceandlove')
        'I wish you all and !'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub(r'\B(\#[a-zA-Z]+\b)(?!;)', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_hashtags(self, text:str, placeholder:str):
        """Replaces hashtags from text by a given placeholder

        >>> replace_hashtags('I wish you all #love and #peace', 'HASHTAG')
        'I wish you all HASHTAG and HASHTAG'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the hashtag
        """
        text = re.sub(r'\B(\#[a-zA-Z]+\b)(?!;)', placeholder, text)
        return text

    def remove_ips(self, text:str):
        """Removes ips from text

        >>> remove_ips('This can be accessed on the address 198.162.0.1.')
        'This can be accessed on the address .'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_ips(self, text:str, placeholder:str):
        """Replaces ips from text by a given placeholder

        >>> replace_ips('This can be accessed on the address 198.162.0.1.', 'IP')
        'This can be accessed on the address IP.'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the ip
        """
        text = re.sub(r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}', placeholder, text)
        return text

    def remove_handles(self, text:str):
        """Removes handles from text

        >>> remove_handles('Can you come tonight, @alice?')
        'Can you come tonight, ?'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub(r'\B(\@[a-zA-Z_0-9]+\b)(?!;)', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_handles(self, text:str, placeholder:str):
        """Replaces handles from text by a given placeholder

        >>> replace_handles('Can you come tonight, @alice?', 'USERNAME')
        'Can you come tonight, USERNAME?'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the handle
        """
        text = re.sub(r'\B(\@[a-zA-Z_0-9]+\b)(?!;)', placeholder, text)
        return text

    def strip_accents(self, text:str):
        """Strip accents from tokens

        >>> strip_accents('Mamãe me disse: você é o menino mais ágil do time.')
        'Mamae me disse: voce e o menino mais agil do time.'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        try:
            text = unicode(text, 'utf-8')
        except NameError: 
            pass

        text = unicodedata.normalize('NFD', text)\
            .encode('ascii', 'ignore')\
            .decode("utf-8")

        return str(text)

    def replace_quotes(self, text:str, placeholder:str):
        """Replaces quotes by a placeholder

        >>> replace_quotes('She told me: "you are a confident and strong woman".', 'QUOTED')
        'She told me: QUOTED you are a confident and strong woman QUOTED .'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the quote sign
        """
        text = re.sub(r'"', ' ' + placeholder + ' ', text)
        text = re.sub(r"'", ' ' + placeholder + ' ', text)
        text = self.remove_excessive_spaces(text)
        return text

    def remove_punctuation(self, text:str):
        """Removes ponctuation and special chars

        >>> remove_punctuation('Hey, are you here??? I really need your help!')
        'Hey are you here I really need your help'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the punctuation and the special chars found
        """
        text = re.sub(r'[^\w\s]', ' ', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_punctuation(self, text:str, sign='', placeholder=''):
        """Replaces punctuation by a placeholder.

        >>> replace_punctuation('Hey, are you here??? I really need your help!', sign='?', placeholder='QUESTION')
        'Hey, are you here QUESTION QUESTION QUESTION I really need your help!'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        sign : str
            The symbol you want to replace. If no symbol is provided, will replace all special chars and punctuation by the placeholder
        placeholder : str
            The string that will replace the punctuation and the special chars found
        """
        if sign is '':
            text = re.sub(r'[^\w\s]', ' ' + placeholder + ' ', text)    
        else:
            text = text.replace(sign, ' ' + placeholder + ' ')
        
        text = self.remove_excessive_spaces(text)
        return text

    def remove_numbers(self, text:str):
        """Removes all numbers

        >>> remove_numbers('I told him 1,2,3,4 times that he could not do that!')
        'I told him ,,, times that he could not do that!'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub(r'[0-9]', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_numbers(self, text:str):
        """Replaces all numbers for string

        >>> replace_numbers('I told him 1,2,3,4 times that he could not do that!')
        'I told him one, two, three, four times that he could not do that!'. 
        Attention: It does NOT convert numbers like 20 to twenty, but to
        two zero.

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        if self.language == 'pt-br':
            text = text.replace('0', ' zero ')
            text = text.replace('1', ' um ')
            text = text.replace('2', ' dois ')
            text = text.replace('3', ' três ')
            text = text.replace('4', ' quatro ')
            text = text.replace('5', ' cinco ')
            text = text.replace('6', ' seis ')
            text = text.replace('7', ' sete ')
            text = text.replace('8', ' oito ')
            text = text.replace('9', ' nove ')
        elif self.language == 'fr':
            text = text.replace('0', ' zéro ')
            text = text.replace('1', ' un ')
            text = text.replace('2', ' deux ')
            text = text.replace('3', ' trois ')
            text = text.replace('4', ' quatre ')
            text = text.replace('5', ' cinc ')
            text = text.replace('6', ' six ')
            text = text.replace('7', ' sept ')
            text = text.replace('8', ' huit ')
            text = text.replace('9', ' neuf ')
        elif self.language == 'en':
            text = text.replace('0', ' zero ')
            text = text.replace('1', ' one ')
            text = text.replace('2', ' two ')
            text = text.replace('3', ' three ')
            text = text.replace('4', ' four ')
            text = text.replace('5', ' five ')
            text = text.replace('6', ' six ')
            text = text.replace('7', ' seven ')
            text = text.replace('8', ' eight ')
            text = text.replace('9', ' nine ')
            
        text = self.remove_excessive_spaces(text)
        return text

    def stem_sentence(self, text:str, tagged_text=False, sep='_'):
        """Uses NLTK stemmer to stem tokens

        >>> stem_sentence('I love to go shopping with my mother and friends')
        'i love to go shop with my mother and friend'

        Parameters
        ----------
        text : str
            The string that will be transformed
        tagged_text : boolean
            If you are using a text that was previously tagged, set it to True
        sep : str
            Here you can inform wich separator was used to merge the word to the tag
        """
        stemmer = SnowballStemmer(self.nltk_language)
        sentence = []
        words_tags = []
        words = []
        tags = []

        # removes more than one whitespace 
        text = re.sub(r'\s{2,}', ' ', text)

        if tagged_text == True:
            
            for tagged_word in text.split(' '):
                for tokens in tagged_word.split(sep):
                    words_tags.append(tokens)

            for i, token in enumerate(words_tags):
                if i%2 == 0:
                    words.append(stemmer.stem(str(token)))
                    tags.append(words_tags[i+1])

            # recompose sentence    
            for i, word in enumerate(words):
                sentence.append(word+sep+tags[i])

            text = ' '.join(sentence)
    
        else:

            for word in text.split(' '):
                word = stemmer.stem(str(word))
                sentence.append(word)

            text = ' '.join(sentence)
        
        return text

    def remove_stopwords(self, text:str, custom_list=[], extend_set=False):
        """ Removes stopwords

        >>> remove_stopwords('I love to go shopping with my mother and friends')
        'love go shopping mother friends'

        Parameters
        ----------
        text : str
            The string that will be transformed
        custom_list : list
            A list with the custom stopwords list
        extend_set : boolean
            Use this option to extend the stopwords of a given language with your custom list 
        """
        # define the stopwords set to use
        if self.language == 'pt-br':
            stop_words = stopwords.portuguese
        elif self.language == 'en':
            stop_words = stopwords.english
        elif self.language == 'fr':
            stop_words = stopwords.french
        elif self.language == 'custom':
            stop_words = custom_list
        else:
            print('This set is not available. No stopword will be removed!')
            stop_words = []

        # extend with a list of custom words
        if extend_set == True:
            stop_words = stop_words + custom_list

        # add spaces to the beginning and to the end of the sentence, 
        # so stopwords on these locations can be spotted
        text = ' ' + text + ' '

        for word in stop_words:
            text = re.sub(r'\s' + word + r'\s', ' ', text, flags=re.I)
            if self.language == 'fr':
                text = re.sub(re.escape('j\''), ' ', text, flags=re.I)
                text = re.sub(re.escape('d\''), ' ', text, flags=re.I)
                text = re.sub(re.escape('l\''), ' ', text, flags=re.I)
                text = re.sub(re.escape('m\''), ' ', text, flags=re.I)
                text = re.sub(re.escape('s\''), ' ', text, flags=re.I)
                text = re.sub(re.escape('t\''), ' ', text, flags=re.I)
            
        # strip adititonal the spaces
        text = text.strip()
        text = self.remove_excessive_spaces(text)
        return text

    def reduce_words_with_repeated_chars(self, text:str):
        """Reduces words with chars repeated more than 3 times to a single char. 
        Useful to replace words such as loooooooong by long. Be careful, 
        as it can change abreviations such as AAA to single A 

        >>> reduce_words_with_repeated_chars('I loooooooooooooove pizza so muuuchhhh')
        'I love pizza so much'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        findings = re.findall(r'(\w)\1{2,}', text)
        for char in findings:
            find = char + '{3,}'
            #replace = char + '\1' + '???'
            replace = '???' + char + '???'
            text = re.sub(find, repr(replace), text)

        # Now we can remove the placeholders    
        text = text.replace('\'???','')
        text = text.replace('???\'','')

        text = self.remove_excessive_spaces(text)
        return text
        
    def remove_excessive_spaces(self, text:str):
        """Removes excessive space from text

        >>> remove_excessive_spaces('I  can\'t     stop looking at  you')
        'I can't stop looking at you'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        # remove more than one space
        text = re.sub(r'(\s)\1{1,}', ' ', text)
        # remove spaces in the beginning and in the end of the string
        text = text.strip()
        return text

    def merge_same_tokens(self, text:str):
        """If two tokens are the same, this will merge them. Specially useful when dealing with social
        media text

        >>> merge_same_tokens("eu acho q vc tb estava lá")
        'eu acho que você também estava lá'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """

        if self.language == 'pt-br':
            text = ' ' + text + ' '
            for k,v in same_words.pt_br.items():
                text = re.sub(re.escape(k), v, text, flags=re.I)
            text = text.strip()
            
        return text
        
    def expand_contractions(self, text:str):
        """ Expands some contractions and merge same tokens
        
        >>> expand_contractions("you're solely responsible for your success and your failure.")
        'you are solely responsible for your success and your failure.'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """

        if self.language == 'en':
           # Expand contractions
            text = ' ' + text + ' '
            for k,v in same_words.en.items():
                text = re.sub(re.escape(k), v, text, flags=re.I)                    
            text = text.strip()
        
        elif self.language == 'fr':
            # Expand contractions
            text = ' ' + text + ' '
            for k,v in same_words.fr.items():
                text = re.sub(re.escape(k), v, text, flags=re.I)
            text = text.strip()

        elif self.language == 'pt-br':
            # Expand contractions
            text = ' ' + text + ' '
            for k,v in same_words.pt_br.items():
                text = re.sub(re.escape(k), v, text, flags=re.I)
            text = text.strip()

        return text

    def lower_remove_white(self, text:str):
        """ Use it to lower text and to remove training whitespaces
        
        >>> lower_remove_white("- Mary is coming! \n- When? \n- Today!")
        '- mary is coming!  - when?  - today!'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        
        # lower text
        text = text.lower()
        # strip trailing whitespaces
        text = re.sub(r'\n', ' ', text)

        return text

    def preprocess(self, text:str, tokenize=True, stem=True, remove_stopwords=False, tag_text=False):
        """Preprocess text before data handling with most common settings.
        Text is processed in the following order:

            - lower text
            - strip trailing whitespaces
            - convert emojis to text
            - expand contractions
            - replace urls
            - remove tags
            - replace hashtags
            - replace ips
            - remove social media handles
            - replace numbers
            - reduce loooooong words
            - remove punctuation
            - remove excessive spaces     
            - tag text if True
            - remove stopwords if True
            - remove accents
            - remove excessive spaces
            - stem text if True
            - tokenize text (if True, will return a list of tokens)

        >>> preprocess("At the end of the day, you're solely responsible for your success and your failure. And the sooner you realize that, you accept that, and integrate that into your work ethic, you will start being successful. As long as you blame others for the reason you aren't where you want to be, you will always be a failure.")
        '['end', 'day', 'sole', 'respons', 'success', 'failur', 'sooner', 'realiz', 'that', 'accept', 'that', 'integr', 'work', 'ethic', 'start', 'success', 'long', 'blame', 'other', 'reason', 'want', 'be', 'alway', 'failur']'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        tokenize : boolean
            Defines if the text should be tokenized or not
        stem : boolean
            Defines if the tokens should be stemmed or not
        remove_stopwords : boolean
            Defines if stopwords should be removed.
        tag_text : boolean
            Defines if text should be tagged
        """
        text = str(text)
        # lowers text and removes trailing spaces
        text = self.lower_remove_white(text)
        # emoji to text
        text = emoji.demojize(text)
        # expands contractions
        text = self.expand_contractions(text)
        # replace urls
        text = self.replace_urls(text, 'URL')
        # remove tags
        text = self.remove_html_tags(text)
        # replace hashtags
        text = self.replace_hashtags(text, 'HASHTAG')
        # replace ips
        text = self.replace_ips(text, 'IP')
        # remove social media handles
        text = self.remove_handles(text)
        # remove numbers
        text = self.replace_numbers(text)
        # reduce loooooong words
        text = self.reduce_words_with_repeated_chars(text)
        # remove punctuation        
        text = self.remove_punctuation(text)
        # remove excessive spaces
        text = self.remove_excessive_spaces(text)
        # remove stopwords
        if remove_stopwords:
            text = self.remove_stopwords(text)
        # tags the text. 
        if tag_text:
            annotated_sentence = self.tag_tokens(text=text, sep='taggedtext')
            # retransform into sentence
            text = ' '.join(annotated_sentence)
        # remove accents        
        text = self.strip_accents(text)
        # remove excessive spaces
        text = self.remove_excessive_spaces(text)
        # stem text
        if stem:
            text = self.stem_sentence(text, tagged_text=tag_text, sep='taggedtext')
        # tokenize
        if tokenize:
            text = self.tokenize(text)
            self.vocab.update(text)
        else:        
            # updates list of words
            self.vocab.update(list(text))
        # remove the tagged mark if tag is True
        if tag_text == True and tokenize == False:
            text = text.replace('taggedtext', '_')
        elif tag_text == True and tokenize == True:
            text = [t.replace('taggedtext', '_') for t in text]
        # return text
        return text
            
    def preprocess_list(self, text_list:list, tokenize=True, stem=True, remove_stopwords=False, tag_text=False):
        """Preprocess every text in a list of strings

        >>> list_of_strings = [
        'I love you',
        'Please, never leave me alone',
        'If you go, I will die',
        'I am watching a lot of romantic comedy lately',
        'I have to eat icecream'
        ]

        >>> list_processed = aruana_en.preprocess_list(list_of_strings, stem=False)
        '['end', 'day', 'sole', 'respons', 'success', 'failur', 'sooner', 'realiz', 'that', 'accept', 'that', 'integr', 'work', 'ethic', 'start', 'success', 'long', 'blame', 'other', 'reason', 'want', 'be', 'alway', 'failur']'

        Parameters
        ----------
        text : list
            The list of strings to be transformed
        tokenize : boolean
            Defines if the text should be tokenized or not
        stem : boolean
            Defines if the tokens should be stemmed or not
        remove_stopwords : boolean
            Defines if stopwords should be removed.
        tag_text : boolean
            Defines if text should be tagged
        """
        texts = []
        for text in tqdm(text_list):
            # preprocess
            text = self.preprocess(text, tokenize=tokenize, stem=stem, remove_stopwords=remove_stopwords, tag_text=tag_text)
            texts.append(text)
        return texts

    def create_corpus(self, text_list:list):
        """Receives a list of strings and returns a single string with all the text

        >>> list_of_strings = ['I love you', 'Please, never leave me alone', 'If you go, I will die', 'I am watching a lot of romantic comedy lately', 'I have to eat icecream']
        >>> create_corpus(list_of_strings)
        'I love you Please, never leave me alone If you go, I will die I am watching a lot of romantic comedy lately I have to eat icecream'

        Parameters
        ----------
        text : list
            The list of strings that will be used to return the corpus
        """
        corpus = ' '
        for text in text_list:
            corpus = corpus + ' ' + str(text)
        return corpus

    def vocab_size(self, corpus:str):
        """Returns the number of unique tokens found on the corpus

        >>> vocab_size('  I love you Please, never leave me alone If you go, I will die I am watching a lot of romantic comedy lately I have to eat icecream')
        24

        Parameters
        ----------
        corpus : str
            The string that holds the entire corpus
        """
        words_map_dict = Counter(corpus.split())
        unique_words = len(words_map_dict.keys())
        vocab_size = int(unique_words)

        return vocab_size

    def lexical_diversity(self, corpus_slice:str, corpus:str):
        """Counts the vocab of one part of the corpus and compares it to the entire corpus

        >>> lexical_diversity(list_of_strings[0], corpus)
        0.125

        Parameters
        ----------
        corpus_slice : str
            The string that you want to analyse
        corpus : str
            The string that holds the entire corpus
        """
        # count vocab for slice 
        slice_vocab = self.vocab_size(corpus_slice)

        # count vocab for entire corpus
        all_vocab = self.vocab_size(corpus)

        # calculate diversity
        diversity = slice_vocab/all_vocab
        return diversity

    def random_classification(self, list_to_classify:list, classes:list, balanced=True):
        '''Receives a list of sentences and assigns randomly a label for each item
        on the list. Returns a list with all the random labels.
        
        >>> random_classification(['A sentence', 'Another sentence'][0,1])
            [1,1]

        Parameters
        ----------
        list_to_classify : list
            A list that holds all the text that needs to be randomly labelized
                
        classes : list
            A list that holds the possible classes

        balanced : boolean
            If True, resulting labels will have an equal or near equal number of examples
        '''
        
        labels = []

        if balanced == False:
            # iterates over the list and picks a random label
            for sentence in list_to_classify:
                labels.append(random.choice(classes))
        else:
            examples_len = len(list_to_classify)
            classes_len = len(classes)
            examples_per_class = math.ceil(examples_len/classes_len)

            # initializes counter 
            class_counter = {}
            for available_label in classes:
                class_counter[available_label] = 0

            # iterates over the list and picks a random label
            for sentence in list_to_classify:
                random_class = random.choice(classes)
                # check if class is already completed
                if class_counter[random_class] <= examples_per_class:
                    # adds label to labels list
                    labels.append(random_class)
                    # updates counter
                    class_counter[random_class] += 1
                else:
                    # remove classwih enough values from list
                    classes.remove(random_class)
                    # picks a new random value and adds to final labels list
                    labels.append(random.choice(classes))

            # print final number of examples
            for k, v in class_counter.items():
                print('Class {}: {} values'.format(k, v))

        return labels

    def replace_with_blob(self, list_of_texts:list):
        '''Receives a list of sentences and creates blobs with the same 
        vocabulary found on corpus.
        
        >>> random_classification(['A sentence', 'Another sentence'][0,1])
            ['sentence Another', 'A Another']

        Parameters
        ----------
        list_of_texts : list
            A list that holds all the text that needs to become blob
        '''
        
        # create vocab list
        vocab = list(self.create_corpus(list_of_texts).split(' '))

        # create lists that will hold the sentences
        sentence = []
        sentences = []
        
        for text in list_of_texts:
            # get the lenght for each text on the list
            text_len = len(str(text).split(' ')) 
            
            # iterate over each text and replace token by a token from vocab
            for x in range(text_len):
                sentence.append(random.choice(vocab))

            # recreate sentence and clean sentence list
            text = ' '.join(sentence)
            sentence = []

            # append recreated sentence to list
            sentences.append(text)

        return sentences

    def tokenize(self, text:str):
        '''Receives a string and splits it into tokens.
        Returns a list with all the tokens found.

        >>> tokenize('I want to eat mow.")
            ['I', 'want', 'to', 'eat', 'now', '.']

        Parameters
        ----------
        text : str
            The text that will be tokenized
        '''
        
        # transform ellipses into real ellipses
        text = text.replace('...', '…')

        # keep urls together
        url_results = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text, re.MULTILINE)
        
        # replace urls for a moment
        text = self.replace_urls(text, 'URLHOLDER')

        # insert extra spaces before and after emojis
        for p in emoji.UNICODE_EMOJI:
            text = text.replace(p, ' ' + p + ' ')

        # insert extra spaces before and after punctuation
        for p in strings.punctuation:
            text = text.replace(p, ' ' + p + ' ')
    
        # remove extra spaces
        text = self.remove_excessive_spaces(text)

        # replace urls by the real urls
        for url in url_results:
            text = text.replace('URLHOLDER', url, 1) 

        # tokenize using spaces
        tokens = []
        for token in text.split(' '):
            if token != ' ' or token != '':
                tokens.append(token)
            
        # return list of tokens  
        return tokens

    def tag_tokens(self, text:str, merge_tags=True, sep="_"):
        """Tags each token on a list of tokens and merges their tags to the original token if merge_tags=True
        Parameters
        ----------
        text : str
            A list that holds all the tokens to be tagged
        merge_tags : boolean
            If True, will merge tages to the original tokens. If false, will return a list of tuples
        sep : str
            The symbol you want to use to merge the token with its tag
        """
        # hold the merged tokens somewhere
        tagged_sentence = self.annotate([text])
        tagged_sentence = tagged_sentence[0]
        
        if merge_tags == True:
            # loops through the tagged sentence and merges the token to its tag
            merged = []
            for ensemble in tagged_sentence:
                merged_token = ensemble[0] + sep + ensemble[1]
                merged.append(merged_token)

            return merged

        else:
            return tagged_sentence

    #####################################################################################
    #                                                                                   #
    #                                   ANNOTATOR                                       #
    #                                                                                   #
    #####################################################################################

    def ingest(self, path, text_column = ''):
        """Reads a csv file and returns the corpora in the form of a list
        of sentences.
        
        Parameters
        ----------
        path : str
            The place where your corpora is saved 
        text_column : ''
            The name of the column that holds the text
        """

        # load data
        data = pd.read_csv(path)

        # remove empty fields
        data = data.dropna(subset=[text_column])

        # get only texts in the form of a list
        data = list(data[text_column])

        return data

    def _save_object(self, object_name:str, object_to_save, path='', model_name='', model_version='', language='',):

        """Receives an Python object and saves it to Aruana work space
        using pickle
        
        Parameters
        ----------
        object_name : str
            The name of the object to be saved
        object_to_save
            The object you want to save
        path : str
            The path where you want to save your model. Default value is 
            Aruana workspace
        model_name : str
            The name of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        model_version : str
            The version of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        language : str
            The language of the model. Default is the one you choosed 
            when Aruana was started
        """
        # defines the path, the name and the language of the model to be saved
        path, model_name, model_version, language = self._path_model_language(path=path, 
                                                              model_name=model_name, 
                                                              model_version=model_version,
                                                              language=language)
        
        # check if system has already the Aruana directory
        if os.path.isdir(self.path) == True:
            # saves object
            with open('{}{}_{}_{}_{}'.format(path, object_name, model_name, model_version, language), 'wb') as file:
                pickle.dump(object_to_save, file, protocol=pickle.HIGHEST_PROTOCOL)
                print('Object saved')
        # if false, create the directory
        else:
            try:
                access_rights = 0o755
                os.makedirs(self.path, access_rights)
                # now save it
                with open('{}{}_{}_{}_{}'.format(path, object_name, model_name, model_version, language), 'wb') as file:
                    pickle.dump(object_to_save, file, protocol=pickle.HIGHEST_PROTOCOL)
                    print('Object saved')
            except OSError:  
                print ("Creation of the directory %s failed" % path)
            else:  
                print ("Successfully created the directory %s" % path)

    def _load_models(self, path='', model_name='', model_version='', language='',):
        """Loads a previously saved model
        
        Parameters
        ----------
        path : str
            The path of the model
        model_name : str
            The name of the model
        model_version : str
            The version of the model
        language : str
            The language of the model
        """
        path, model_name, model_version, language = self._path_model_language(path=path, model_name=model_name, model_version = model_version, language=language)

        try:
            clf = load_model('{}{}_{}_{}'.format(path, model_name, model_version, language))
            return clf
        except:
            warnings.warn('Could not load model. Execution will continue, but you can\'t do annotations unless you download the necessary models.', Warning)

    def _load_object(self, object_name:str):
        """Loads a Python object saved on the Aruana work space on pickle format
        
        Parameters
        ----------
        object_name : str
            The name of the object to be loaded
        """

        try:
            filename = '{}{}_{}_{}_{}'.format(self.path, object_name, self.model_type, self.model_version, self.language)
            with open(filename, 'rb') as file:
                return pickle.load(file) 
                
        except Exception as e:
            loaded_object = ''
            return loaded_object
            print('It was not possible to load the object because:',e)

    def _path_model_language(self, path='', model_name='', model_version='', language=''):
        """Defines the path, the model and the language to save or load a model

        Parameters
        ----------
        path : str
            The path of the model
        model_name : str
            The name of the model
        model_version : str
            The version of the model
        language : str
            The language of the model
        """

        if path is not '':
            path = path
        else:
            path = self.path

        if model_name is not '':
            model_name = model_name
        else:
            model_name = self.model_type

        if model_version is not '':
            model_version = model_version
        else:
            model_version = self.model_version

        if language is not '':
            language = language
        else:
            language = self.language

        return path, model_name, model_version, language 

    def train_annotations(self, 
                          annotated_sentences:list, 
                          path='',
                          model_name='',
                          model_version='',
                          language='',
                          embedding_output_dim=128,
                          lstm_units=256,
                          save_model=True, 
                          batch_size=128,
                          epochs=40,
                          validation_split=0.2):
        """Trains a new annotation model on a set of annotated tokens. A new model
        will be created and saved on Aruana workspace, erasing any previous model
        saved with the same name. The local where you want to save your model, the 
        name and the language can be changed.
        
        Parameters
        ----------
        annotated_sentences : list
            A list with the sentences that will be used to create the model
        path : str
            The path where you want to save your model. Default value is 
            Aruana workspace
        model_name : str
            The name of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        model_version : str
            The version of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        language : str
            The language of the model. Default is the one you choosed 
            when Aruana was started
        embedding_output_dim : int
            Dimension of the dense embedding
        lstm_units : int
            Dimensionality of the output space for the LSTM Keras layer
        save_model : boolean
            If True, will save the model each time it improves after an epoch. If 
            False, won't save the model at all
        batch_size : int
            The number of samples that will be propagated through the network
        epochs : int
            The number of full training cycles on the training set. 
        validation_split : float
            A value between 0 and 1 that defines how many examples will be used
            in validation
        """

        # receives the tagged sentences and separates the content from the tags
        sentences, sentence_tags =[], [] 

        for tagged_sentence in annotated_sentences:
            sentence, tags = zip(*tagged_sentence)
            sentences.append(np.array(sentence))
            sentence_tags.append(np.array(tags))

        # split the data in training and testing data
        (train_sentences, 
        test_sentences, 
        train_tags, 
        test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)

        # converts the sentences into integers using the internal dictionary.
        (train_sentences_X, 
        test_sentences_X, 
        train_tags_y, 
        test_tags_y, 
        word2index,
        tag2index) = self._word_to_int_dict(train_sentences, test_sentences, train_tags, test_tags)

        # defines the path, the name and the language of the model to be saved
        path, model_name, model_version, language = self._path_model_language(path=path, 
                                                              model_name=model_name, 
                                                              model_version=model_version,
                                                              language=language)

        # save the word2index for later prediction use
        self._save_object(object_to_save = word2index, object_name='word2index', path=path, model_name=model_name, model_version=model_version, language=language)

        # save the tag2index for later prediction use
        self._save_object(object_to_save = tag2index, object_name='tag2index', path=path, model_name=model_name, model_version=model_version, language=language)
        
        # create the model architecture
        model = Sequential()
        model.add(InputLayer(input_shape=(self.maxlen, )))
        model.add(Embedding(len(word2index), embedding_output_dim))
        model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))
        model.add(TimeDistributed(Dense(len(tag2index))))
        model.add(Activation('softmax'))

        model.compile(loss='categorical_crossentropy',
              optimizer=Adam(0.001),
              metrics=['accuracy'])
        
        # prints the model summary
        model.summary()

        # converts the tags into categorical values
        cat_train_tags_y = self._to_categorical(train_tags_y, len(tag2index))

        # creates a checkpointer to save the model after each improvement
        checkpointer = ModelCheckpoint(filepath='{}{}_{}_{}'.format(path, model_name, model_version, language),
                                       verbose = 1,
                                       save_best_only = save_model)

        # trains the model
        history = model.fit(train_sentences_X, 
                  self._to_categorical(train_tags_y, len(tag2index)), 
                  batch_size=batch_size, 
                  epochs=epochs, 
                  validation_split=validation_split,
                  callbacks = [checkpointer],
                  verbose = 1,
                  shuffle = True)

        # evaluates the model 
        scores = model.evaluate(test_sentences_X, self._to_categorical(test_tags_y, len(tag2index)))
        print("{}: {}".format(model.metrics_names[1], scores[1] * 100)) 

        # plots loss and epochs to check underfitting or overfitting
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('model train vs validation loss')
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.legend(['train', 'validation'], loc='upper right')
        plt.show()

        return model

    def _to_categorical(self, sequences:list, categories:int):
        """ Transforms the sequences of tags to sequences of categorical values
        
        Parameters
        ----------
        sequences : list
            A list with the tags found on the set
        categories : int
            The number of categories present on the set
        """
        cat_sequences = []
        for s in sequences:
            cats = []
            for item in s:
                cats.append(np.zeros(categories))
                cats[-1][item] = 1.0
            cat_sequences.append(cats)
        
        return np.array(cat_sequences)

    def _word_to_int_dict(self, 
                         train_sentences:list, 
                         test_sentences:list, 
                         train_tags:list, 
                         test_tags:list):
        """ Creates a dictionary with the tokens and assign an int to each token
        
        Parameters
        ----------
        train_sentences : list
            A list with the training sentences
        test_sentences : list
            A list with the test sentences
        train_tags : list
            A list with the training tags
        test_tags : list
            A list with the test tags
        """
        words, tags = set([]), set([])
 
        for s in train_sentences:
            for w in s:
                words.add(w.lower())
        
        for ts in train_tags:
            for t in ts:
                tags.add(t)
        
        word2index = {w: i + 2 for i, w in enumerate(list(words))}
        word2index['PAD'] = 0  # value used for padding
        word2index['OOV'] = 1  # value used for OOVs
        
        tag2index = {t: i + 1 for i, t in enumerate(list(tags))}
        tag2index['PAD'] = 0  # value used to padding

        # Convert the word dataset to integer dataset, both the words and the tags.  	
        train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []
        
        for s in train_sentences:
            s_int = []
            for w in s:
                try:
                    s_int.append(word2index[w.lower()])
                except KeyError:
                    s_int.append(word2index['OOV'])
        
            train_sentences_X.append(s_int)
        
        for s in test_sentences:
            s_int = []
            for w in s:
                try:
                    s_int.append(word2index[w.lower()])
                except KeyError:
                    s_int.append(word2index['OOV'])
        
            test_sentences_X.append(s_int)
        
        for s in train_tags:
            train_tags_y.append([tag2index[t] for t in s])
        
        for s in test_tags:
            test_tags_y.append([tag2index[t] for t in s])

        # Define the max length sentence for padding 
        self.maxlen = len(max(train_sentences_X, key=len))
 
        train_sentences_X = pad_sequences(train_sentences_X, maxlen=self.maxlen, padding='post')
        test_sentences_X = pad_sequences(test_sentences_X, maxlen=self.maxlen, padding='post')
        train_tags_y = pad_sequences(train_tags_y, maxlen=self.maxlen, padding='post')
        test_tags_y = pad_sequences(test_tags_y, maxlen=self.maxlen, padding='post')

        return train_sentences_X, test_sentences_X, train_tags_y, test_tags_y, word2index, tag2index

    def _logits_to_tokens(self, sequences:list, index:int):
        """Converts the integers back to the tags
        
        Parameters
        ----------
        sequences : list
            A list with the sequences
        index : int
            The index value
        """
        token_sequences = []
        for categorical_sequence in sequences:
            token_sequence = []
            for categorical in categorical_sequence:
                token_sequence.append(index[np.argmax(categorical)])
    
            token_sequences.append(token_sequence)
    
        return token_sequences

    def annotate(self, list_to_predict:list):
        """Does the annotations on a new unseen list of sentences
        using a previously saved model. 
        
        Parameters
        ----------
        list_to_predict : list
            The list with the sentences to be annotated
        """
   
        # load necessary models and objects
        if self.model_type == 'tagger':
            model = self.pos_tagger_model
        word2index = self._load_object('word2index')
        tag2index = self._load_object('tag2index')

        # iterates over the list of sentences to predict and converts the token to int
        list_to_predict_X = []

        for sentence in list_to_predict:
            s_int = []
            sentence = self.tokenize(sentence)
            for token in sentence:
                try:
                    s_int.append(word2index[token.lower()])
                except KeyError:
                    s_int.append(word2index['OOV'])
            list_to_predict_X.append(s_int)
        
        # pad the senquences
        list_to_predict_X = pad_sequences(list_to_predict_X, maxlen=model.input_shape[1], padding='post')

        # get the actual predictions
        predictions = model.predict(list_to_predict_X)

        # converts the predictions from int to tokens
        tags = self._logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})

        # zip the tags with the tokens
        return_predictions = []

        for i, sentence in enumerate(list_to_predict):
            sentence = self.tokenize(sentence)
            tagged = zip(sentence, tags[i])
            return_predictions.append(list(tagged))

        return return_predictions

    def download(self, model_name='', model_version='', language=''):
        
        """Downloads and saves the models necessary for Annotation execution
        
        Parameters
        ----------
        model_name : str
            The name of the model
        model_version : str
            The version of the model. Must be in the format x_x_x.
        language : str
            The language of the model
        """

        path, model_name, model_version, language = self._path_model_language(path='', 
                                                              model_name=model_name, 
                                                              model_version=model_version,
                                                              language=language)

        # check if system already has the Aruana directory
        if os.path.isdir(path) == False:
            try:
                access_rights = 0o755
                os.makedirs(path, access_rights)
            except OSError:  
                print ("Creation of the directory %s failed" % path)
            else:  
                print ("Successfully created the directory %s" % path)

        # save the model accordingly to the type of object
        if model_name == 'tagger' or model_name == 'lemmatizer':
            try:

                # name of the file
                models_zipped_filename = model_name + '_' + model_version + '_' + language + '.zip'

                # online location of the model               
                models_zipped = 'http://nheeng.com/aruana/models/' + model_name + '/' + models_zipped_filename
                
                # Download the model and unzip files
                print('Downloading the compressed model files')
                urllib.request.urlretrieve(models_zipped, path + models_zipped_filename) 
                print('Decompressing files')
                zip_models = zipfile.ZipFile(path + models_zipped_filename, 'r')
                zip_models.extractall(path)
                zip_models.close()
                print('Remove zip file')
                os.remove(path + models_zipped_filename)
                print('Files are ready for use. Reload Aruana!')

            except Exception as e:
                print('It was not possible to download the models because:', e)

