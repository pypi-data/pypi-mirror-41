"""Various utilities for analyzers to work with UASTs and plain texts."""
from collections import defaultdict
from difflib import SequenceMatcher
import logging
import random
from typing import Dict, Iterable, List, Optional, Sequence

from bblfsh import Node

from lookout.core.api.service_data_pb2 import File


def find_new_lines(before: File, after: File) -> List[int]:
    """
    Return the new line numbers from the pair of "before" and "after" files.

    :param before: the previous contents of the file.
    :param after: the new contents of the file.
    :return: list of line numbers new to `after`.
    """
    matcher = SequenceMatcher(a=before.content.decode("utf-8", "replace").splitlines(),
                              b=after.content.decode("utf-8", "replace").splitlines())
    result = []
    for action, _, _, j1, j2 in matcher.get_opcodes():
        if action in ("equal", "delete"):
            continue
        result.extend(range(j1 + 1, j2 + 1))
    return result


def find_deleted_lines(before: File, after: File) -> List[int]:
    """
    Return line numbers next to deleted lines in the new file.

    :param before: the previous contents of the file.
    :param after: the new contents of the file.
    :return: list of line numbers next to deleted lines.
    """
    before_lines = before.content.decode("utf-8", "replace").splitlines()
    after_lines = after.content.decode("utf-8", "replace").splitlines()
    matcher = SequenceMatcher(a=before_lines, b=after_lines)
    result = []
    for action, _, _, j1, _ in matcher.get_opcodes():
        if action == "delete":
            if j1 != 0:
                result.append(j1)
            if j1 != len(after_lines):
                result.append(j1 + 1)
    return result


def extract_changed_nodes(root: Node, lines: Sequence[int]) -> List[Node]:
    """
    Collect the list of UAST nodes which lie on the changed lines.

    :param root: UAST root node.
    :param lines: Changed lines, typically obtained via find_new_lines(). Empty list means all \
                  the lines.
    :return: list of UAST nodes which are suspected to have been changed.
    """
    lines = set(lines)
    queue = [root]
    result = []
    while queue:
        node = queue.pop()
        for child in node.children:
            queue.append(child)
        if not node.start_position:
            continue
        if not lines or node.start_position.line in lines:
            result.append(node)
    return result


def files_by_language(files: Iterable[File]) -> Dict[str, Dict[str, File]]:
    """
    Sorts files by programming language and path.

    :param files: iterable of `File`-s.
    :return: dictionary with languages as keys and files mapped to paths as values.
    """
    result = defaultdict(dict)
    for file in files:
        if not len(file.uast.children):
            continue
        result[file.language.lower()][file.path] = file
    return result


def filter_files(files: Dict[str, File], line_length_limit: int, overall_size_limit: int,
                 random_state: int = 7, log: Optional[logging.Logger] = None) -> List[File]:
    """
    Filter files based on their maximum line length.

    :param files: files to filter.
    :param line_length_limit: maximum line length to accept a file.
    :param overall_size_limit: maximum cumulative files size in bytes. \
                               The files are discarded after reaching this limit.
    :param random_state: random generator state for shuffling the files.
    :param log: logger to use to report the number of excluded files.
    :return: files passed through the filter and the number of files which were excluded.
    """
    line_passed = []
    for key, file in files.items():
        if len(max(file.content.splitlines(), key=len, default=b"")) <= line_length_limit:
            line_passed.append(key)
    if log is not None:
        log.debug("excluded %d/%d files by max line length %d",
                  len(files) - len(line_passed), len(files), line_length_limit)
    line_passed.sort()
    random.seed(random_state)
    line_passed = random.sample(line_passed, k=len(line_passed))
    size = 0
    size_passed = []
    for key in line_passed:
        file = files[key]
        size += len(file.content)
        if size > overall_size_limit:
            break
        size_passed.append(file)
    if log is not None:
        log.debug("excluded %d/%d files by max overall size %d",
                  len(line_passed) - len(size_passed), len(line_passed), overall_size_limit)
    return size_passed
