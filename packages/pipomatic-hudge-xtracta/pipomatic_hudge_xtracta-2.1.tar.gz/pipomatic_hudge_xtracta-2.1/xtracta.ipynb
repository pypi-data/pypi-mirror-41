{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Union  # From mypy library\n",
    "\n",
    "# import pipomatic_pipomatic_obfuscate_files as obfuscate\n",
    "# import pipomatic_hudge_po_data_pandas_ellipse as po\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xmltodict  # type: ignore\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from jinja2 import Template\n",
    "from PIL import Image, ImageSequence  # type: ignore\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype, is_numeric_dtype, is_integer_dtype, is_object_dtype, is_string_dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows = 100\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "api_key = os.getenv('XTRACTA_API_KEY')\n",
    "database_id = os.getenv('XTRACTA_DATABASE_ID')\n",
    "header_workflow = os.getenv('XTRACTA_HEADER_ID')\n",
    "line_workflow = os.getenv('XTRACTA_LINE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_p = Path.cwd()\n",
    "test_dp = test_p / 'data'\n",
    "test_sp = test_p / 'test_samples'\n",
    "test_ip = test_p / 'input'\n",
    "test_op = test_p / 'output'\n",
    "test_jp = test_p / 'junk'\n",
    "test_lp = test_p / 'lines'\n",
    "test_od = test_p / 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_file = '20190131_invoice.pdf'\n",
    "\n",
    "obfuscate.delete_files(test_ip)\n",
    "obfuscate.delete_files(test_op)\n",
    "obfuscate.delete_files(test_lp)\n",
    "obfuscate.move_files(test_sp, test_file, test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "df = obfuscate.obfuscate_excel(test_dp / 'Purchase Order Master Data' / 'SP004.xls')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "df = obfuscate.obfuscate_csv(test_dp / 'Stock PO Master Data' / 'SP005.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "df = obfuscate.obfuscate_excel(test_dp / 'Supplier Master Data' / 'SP001.xls')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for interacting with Xtracta's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload file\n",
    "\n",
    "Uploads a PDf or image file for extraction. The classifier field is used if you want to assign a specific classifier to the document rather than letting Xtracta make its own classification decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(api_key, workflow_id, filename, classifier=\"\"):\n",
    "    classifier_xml = (\n",
    "        f'<field_data><field name=\"Classifier\">{classifier}</field></field_data>'\n",
    "    )\n",
    "    upload_url = \"https://api-app.xtracta.com/v1/documents/upload\"\n",
    "    file = {\"userfile\": open(filename, mode=\"rb\")}\n",
    "    data = {\n",
    "        \"api_key\": api_key,\n",
    "        \"workflow_id\": workflow_id,\n",
    "        \"field_data\": classifier_xml,\n",
    "    }\n",
    "    r = requests.post(upload_url, data=data, files=file)\n",
    "    if r.status_code != 200:\n",
    "        print(r.status_code)\n",
    "        return t.text\n",
    "    else:\n",
    "        response = xmltodict.parse(r.text)\n",
    "        return response[\"xml\"][\"document_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_document_id = upload_file(api_key, header_workflow, test_ip / test_file)\n",
    "test_document_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(api_key: str, document_id: str):\n",
    "    \n",
    "    \"\"\"retrieves the full xml document from Xtracta and converts it to a dict\"\"\"\n",
    "    \n",
    "    documents_url = \"https://api-app.xtracta.com/v1/documents\"\n",
    "    data = {\"api_key\": api_key, \"document_id\": document_id}\n",
    "    try:\n",
    "        r = requests.post(documents_url, data=data)\n",
    "        response = xmltodict.parse(r.text)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return e.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_document = get_document(api_key, test_document_id)\n",
    "test_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xtracta_status(\n",
    "    api_key: str,\n",
    "    workflow_id: str,\n",
    "    status: str,\n",
    "    api_download_status: str = \"active\",\n",
    "    detailed: int = 0,\n",
    "    documents_order: str = \"asc\",\n",
    ") -> list:\n",
    "    \"\"\"Returns a list of all Xtracta documents with a particular status\"\"\"\n",
    "    documents_url = \"https://api-app.xtracta.com/v1/documents\"\n",
    "    data = {\n",
    "        \"api_key\": api_key,\n",
    "        \"workflow_id\": workflow_id,\n",
    "        \"document_status\": status,\n",
    "        \"api_download_status\": api_download_status,\n",
    "        \"items_per_page\": 1000,\n",
    "        \"detailed\": detailed,\n",
    "        \"documents_order\": documents_order,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(documents_url, data=data)\n",
    "        response = xmltodict.parse(r.text)\n",
    "    except Exception as e:\n",
    "        return [e.__str__]\n",
    "\n",
    "    try:\n",
    "        response_content = response[\"documents_response\"][\"document\"]\n",
    "        if type(response_content) == list:\n",
    "            return response_content\n",
    "        else:\n",
    "            return [response_content]\n",
    "    except Exception as e:\n",
    "        if type(e).__name__ == \"KeyError\":\n",
    "            return [f\"No {status} documents in queue\"]\n",
    "        else:\n",
    "            return [e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_reject_list = get_xtracta_status(api_key, header_workflow, 'reject')\n",
    "test_reject_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_documents_to_skip(api_key, header_workflow):\n",
    "    \n",
    "    \"\"\"You only want to process documents that have data in the document body. \n",
    "    This function finds documents that are not in this state\"\"\"\n",
    "    \n",
    "    status_to_skip = ['reject', 'preprocessing', 'output-in-progress']\n",
    "    items_to_skip = []\n",
    "    for status in status_to_skip:\n",
    "        queue = get_xtracta_status(api_key, header_workflow, status)\n",
    "        for item in queue:\n",
    "            if item != f'No {status} documents in queue':\n",
    "                items_to_skip.append(item['document_id'])\n",
    "    return items_to_skip      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "find_documents_to_skip(api_key, header_workflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the output dictionary from Xtracta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output(document: Dict[Any, Any]) -> Dict[Any, Any]:\n",
    "    \"\"\"Returns a dictionary with document_id, status and version as top level values \n",
    "    and remaining fields as key value pairs in a header section\"\"\"\n",
    "    output = {}\n",
    "    header_dict = document[\"documents_response\"][\"document\"][\"field_data\"][\"field\"]\n",
    "    header = transform_dict(header_dict)\n",
    "    output[\"document_id\"] = document[\"documents_response\"][\"document\"][\"document_id\"]\n",
    "    output[\"status\"] = document[\"documents_response\"][\"document\"][\"document_status\"]\n",
    "    output[\"version\"] = document[\"documents_response\"][\"document\"][\"@revision\"]\n",
    "    output[\"header\"] = header\n",
    "    return output\n",
    "\n",
    "\n",
    "def transform_dict(start_dict):\n",
    "    end_dict = {}\n",
    "    for item in start_dict:\n",
    "        end_dict[item[\"field_name\"]] = item[\"field_value\"]\n",
    "    return end_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_document = get_document(api_key, test_document_id)\n",
    "test_output = create_output(test_document)\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents_wo_json(folder):\n",
    "    json_files = []\n",
    "    pdfs = []\n",
    "    json_list = list(folder.glob(\"*.json\"))\n",
    "    pdf_list = list(folder.glob(\"*.pdf\"))\n",
    "    for file in json_list:\n",
    "        json_files.append(file.stem)\n",
    "    for pdf in pdf_list:\n",
    "        pdfs.append(pdf.stem)\n",
    "    new_documents = list(set(pdfs) - set(json_files))\n",
    "    return new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "get_documents_wo_json(test_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_document_ui(api_key: str, document_id: str) -> str:\n",
    "    \"\"\"Opens the Xtracta UI to fix and train documents\"\"\"\n",
    "    documents_url = \"https://api-app.xtracta.com/v1/documents/ui\"\n",
    "    data = {\n",
    "        \"api_key\": api_key,\n",
    "        \"document_id\": int(document_id),\n",
    "        \"buttons\": \"output,archive\",\n",
    "        \"no_lockout\": 1,\n",
    "        \"expire\": 86400,\n",
    "    }\n",
    "    r = requests.post(documents_url, data=data)\n",
    "    response = xmltodict.parse(r.text)\n",
    "    return response[\"documents_response\"][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_document = get_document(api_key, test_document_id)\n",
    "test_output = create_output(test_document)\n",
    "display(test_output)\n",
    "if test_output['status'] in ['reject', 'output']:\n",
    "    display(open_document_ui(api_key, test_document_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_dashboard_ui(api_key: str, workflow_id: str) -> str:\n",
    "    \"\"\"Opens the Xtracta UI to fix and train documents\"\"\"\n",
    "    documents_url = \"https://app.mybusinessautomated.net/v1/documents/ui\"\n",
    "    data = {\n",
    "        \"api_key\": api_key,\n",
    "        \"workflow_id\": int(line_workflow),\n",
    "        \"buttons\": \"output,archive\",\n",
    "        \"no_lockout\": 1,\n",
    "        \"expire\": 86400,\n",
    "    }\n",
    "    r = requests.post(documents_url, data=data)\n",
    "    response = xmltodict.parse(r.text)\n",
    "    return response[\"documents_response\"][\"url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "open_dashboard_ui(api_key, header_workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_document(\n",
    "    api_key: str, document_id: str, delete: int = 0, api_download_status: str = \"active\"\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"Updates document on Xtracta\"\"\"\n",
    "    documents_url = \"https://api-app.xtracta.com/v1/documents/update\"\n",
    "    data = {\n",
    "        \"api_key\": api_key,\n",
    "        \"document_id\": int(document_id),\n",
    "        \"delete\": delete,\n",
    "        \"api_download_status\": api_download_status,\n",
    "    }\n",
    "    r = requests.post(documents_url, data=data)\n",
    "    response = xmltodict.parse(r.text)\n",
    "    return response[\"documents_response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "update_document(api_key, test_document_id, api_download_status='active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(document):\n",
    "    lines_dict = document['documents_response']['document']['field_data']['field_set']['row']\n",
    "    lines = []\n",
    "    if len(lines_dict) > 1:\n",
    "        for line_dict in lines_dict:\n",
    "            line = transform_dict(line_dict['field'])\n",
    "            lines.append(line)\n",
    "    else:\n",
    "        line = transform_dict(lines_dict['field'])\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_line_document_id = upload_file(api_key, line_workflow, test_ip / test_file)\n",
    "test_line_document = get_document(api_key, test_line_document_id)\n",
    "test_lines = get_lines(test_line_document)\n",
    "test_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build output once in output status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_out_output(document, output):\n",
    "    output['stem'] = output['header']['filename'].split('.')[0]\n",
    "    output['new_filename'] = f\"{output['header']['supplier_id']}-{output['header']['invoice_number']}\"\n",
    "    output['header']['emaildate'] = get_email_date(output['stem'])\n",
    "    output['document_url'] = document['documents_response']['document']['document_url']\n",
    "    output['image_urls'] = get_image_urls(document['documents_response']['document']['image_url'])\n",
    "    return output\n",
    "\n",
    "def get_email_date(stem):\n",
    "    year = stem[:4]\n",
    "    month = stem[4:6]\n",
    "    day = stem[6:8]\n",
    "    return f\"{year}-{month}-{day}\"\n",
    "\n",
    "def get_image_urls(image_urls):\n",
    "    if type(image_urls) != list:\n",
    "        image_urls = [image_urls]\n",
    "    return image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_document = get_document(api_key, test_document_id)\n",
    "test_output = create_output(test_document)\n",
    "full_test_output = build_out_output(test_document, test_output)\n",
    "full_test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add PO line number to Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match(output, po_lines):\n",
    "    is_po = po_lines['po_number'] == output['header']['po_number']\n",
    "    filtered_pos = po_lines[is_po]\n",
    "    filtered_pos = filtered_pos[['stock_code', 'stock_description', 'line_number']]\n",
    "    filtered_pos_list = filtered_pos.to_dict(orient='records')\n",
    "    filtered_pos = filtered_pos.set_index('stock_code')\n",
    "    filtered_pos_dict = filtered_pos.to_dict(orient='index')\n",
    "    matches = []\n",
    "    for i, line in enumerate(output['lines']):\n",
    "        best_match_line_number = 0\n",
    "        best_match_result = 0\n",
    "        if filtered_pos_dict.get(line['po_item']+'y', '') != '': # remove +'y' for production\n",
    "            output['lines'][i]['po_line_number'] = filtered_pos_dict[line['po_item']]['line_number']\n",
    "        else:\n",
    "            print(line['description'])\n",
    "            for i, po_line in enumerate(filtered_pos_list):\n",
    "                match_result = fuzz.partial_ratio(line['description'], po_line['stock_description'])\n",
    "                if match_result > best_match_result and match_result not in matches:\n",
    "                    best_match_result = match_result\n",
    "                    best_match_line_number = i+1\n",
    "                print(' --', match_result, po_line['stock_description'])\n",
    "            print(best_match_result, best_match_line_number)\n",
    "            print()\n",
    "            output['lines'][i]['po_line_number'] = best_match_line_number\n",
    "            matches.append(best_match_line_number)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4f3dd8010366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstock_po_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_od\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'Stock PO Master Data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstock_po_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_last_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_po_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstock_po_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_po_lines_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_po_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mstock_po_lines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hudge\\desktop\\2019_projects\\pipomatic\\hudge\\env\\lib\\site-packages\\pipomatic_hudge_po_data_pandas_ellipse.py\u001b[0m in \u001b[0;36mbuild_po_lines_dataframe\u001b[1;34m(stock_po_csv)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mcolumn_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_list_po_line_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpo_column_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mstock_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpo_column_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mstock_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"line_number\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpo_number\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"{x}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpo_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpo_number\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"{x}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hudge\\desktop\\2019_projects\\pipomatic\\hudge\\env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3192\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3194\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3196\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hudge\\desktop\\2019_projects\\pipomatic\\hudge\\env\\lib\\site-packages\\pipomatic_hudge_po_data_pandas_ellipse.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mcolumn_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_list_po_line_columns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpo_column_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mstock_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpo_column_mapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mstock_pos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"line_number\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpo_number\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"{x}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline_number\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpo_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstock_pos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpo_number\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"{x}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "stock_po_dir = test_od / 'Stock PO Master Data'\n",
    "stock_po_csv = po.get_last_file(stock_po_dir)\n",
    "stock_po_lines = po.build_po_lines_dataframe(stock_po_csv)\n",
    "stock_po_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull company name and location from filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_company_location(output):\n",
    "    company_extract = re.compile(r'.*\\[<<(.*)>>\\].*')\n",
    "    company_location = company_extract.match(output['header']['email_subject'])[1]\n",
    "    try:\n",
    "        output['company'], output['location'] = company_location.split('-')\n",
    "    except:\n",
    "        output['company'] = company_location\n",
    "        output['location'] = 'NA'\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "full_test_output['header']['email_subject'] = '234 [<<ABC-123>>] Here is a subject'\n",
    "full_test_output = add_company_location(full_test_output)\n",
    "full_test_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write JSON files, create TIFs and move PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_simple(filename, output):\n",
    "    filename = filename.with_suffix('.json')\n",
    "    with open(f\"{filename}\", \"w\") as f:\n",
    "        f.write(json.dumps(output, indent=4))\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "test_json_file = write_json_simple(test_ip / test_output['header']['filename'], test_output)\n",
    "test_json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_from_input(api_key, document, ip, lp, op, jp):\n",
    "    output = create_output(document)\n",
    "    json_source = (ip / output['header']['filename']).with_suffix('.json')\n",
    "    pdf_source = (ip / output['header']['filename']).with_suffix('.pdf')\n",
    "    if document['documents_response']['document']['document_status'] == 'output':\n",
    "        output = build_out_output(document, output)\n",
    "        json_destination = json_destination = (op / output['new_filename']).with_suffix('.json')\n",
    "        if not output['header']['line_count']:\n",
    "            output['header']['line_count'] = 1\n",
    "        if float(output['header']['line_count']) > 1:\n",
    "            json_destination = (lp / output['new_filename']).with_suffix('.json')\n",
    "            pdf_destination = (lp / output['new_filename']).with_suffix('.pdf')\n",
    "            if pdf_source.exists():\n",
    "                pdf_destination.write_bytes(pdf_source.read_bytes())\n",
    "        with open(f\"{json_destination}\", \"w\") as f:\n",
    "            f.write(json.dumps(output, indent=4))\n",
    "        save_tif(output, op)\n",
    "        if json_destination.exists() and json_source.exists():\n",
    "            json_source.unlink()\n",
    "            if pdf_source.exists():\n",
    "                pdf_source.unlink()\n",
    "        return 'File moved to output / lines'\n",
    "    elif document['documents_response']['document']['document_status'] == 'qa':\n",
    "        json_destination = (jp / output['header']['filename']).with_suffix('.json')\n",
    "        json_source.replace(json_destination)\n",
    "        if json_destination.exists() and json_source.exists():\n",
    "            json_source.unlink()\n",
    "            if pdf_source.exists():\n",
    "                pdf_source.unlink()\n",
    "            return 'File moved to junk'\n",
    "    else:\n",
    "        return 'File not moved'\n",
    "    \n",
    "        \n",
    "def create_tif_image(image_urls):\n",
    "    images = []\n",
    "    for i, url in enumerate(image_urls):\n",
    "        r = requests.get(url, stream=True)\n",
    "        if i == 0:\n",
    "            im = Image.open(r.raw)\n",
    "        else:\n",
    "            images.append(Image.open(r.raw))\n",
    "    return im, images\n",
    "\n",
    "def save_tif(output, op):\n",
    "    new_name = (op / output['new_filename']).with_suffix('.tif')\n",
    "    im, images = create_tif_image(output['image_urls'])\n",
    "    im.save(f'{new_name}', save_all=True, append_images=images)\n",
    "    return im, images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [],
   "source": [
    "move_files(test_sp, test_file, test_ip)\n",
    "write_json_simple(test_ip / test_output['header']['filename'], test_output)\n",
    "move_from_input(api_key, test_document, test_ip, test_lp, test_op, test_jp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving files in the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "build"
    ]
   },
   "outputs": [],
   "source": [
    "def clean_input_folder(ip):\n",
    "    json_files = ip.glob('*.json')\n",
    "    pdf_files = ip.glob('*.pdf')\n",
    "    files = list(json_files)\n",
    "    files.extend(list(pdf_files))\n",
    "    all_files = ip.glob('*.*')\n",
    "    for file in all_files:\n",
    "        if file not in files:\n",
    "            file.unlink()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "build"
    ]
   },
   "outputs": [],
   "source": [
    "move_files(test_sp, test_file, test_ip)\n",
    "(test_ip / 'test.png').open('w').write('some text')\n",
    "write_json_simple(test_ip / test_output['header']['filename'], test_output)\n",
    "clean_input_folder(test_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting XML for upload into Xtracta's database\n",
    "\n",
    "Take a list of dicts and format it for uploading to Xtracta's database API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database_data(api_key, database_id, out, refresh):\n",
    "    documents_url = 'https://api-app.xtracta.com/v1/databases/data_add'\n",
    "    data = {'api_key': api_key, 'database_id': int(database_id), 'data': out, 'refresh': refresh}\n",
    "    r = requests.post(documents_url, data=data)\n",
    "    response = xmltodict.parse(r.text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xml_data(supplier_data_dict):\n",
    "    xml_rows = []\n",
    "    for row in supplier_data_dict:\n",
    "        po = {'column': [{'@id': '55261', '#text': f\"{row['po_number']}\"}, \n",
    "                         {'@id': '55264', '#text': f\"{row['supplier_number']}\"},\n",
    "                         {'@id': '60223', '#text': f\"{row['line_number']}\"},\n",
    "                         {'@id': '58133', '#text': f\"{row['abn']}\"},\n",
    "                         {'@id': '58134', '#text': f\"{row['bsb']}\"},\n",
    "                         {'@id': '58135', '#text': f\"{row['bank_account']}\"},\n",
    "                         {'@id': '58242', '#text': f\"{row['supplier_name']}\"}]}\n",
    "        xml_rows.append(po)\n",
    "    xml_data = {'xml': {'row': xml_rows}}\n",
    "    return xmltodict.unparse(xml_data, pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put PO line numbers in lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_match(output, po_lines):\n",
    "    \"\"\"\n",
    "    Matches each line in an invoice with the closest match from a PO. The match is first attempted by stock_code.\n",
    "    If that fails, it attempts a fuzzy match against description.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    output: dict\n",
    "        The output is a dict of the invoice. It contains a lines element that holds lines. Each line has a po_item \n",
    "        that matches the stock_code column in the po_lines dataframe. \n",
    "        \n",
    "    po_lines: dataframe\n",
    "        po_lines is a dataframe that lists the po_lines from the PO linked to the invoice. The dataframe has a stock_code\n",
    "        column and a stock_description column that is used to match to the po_item and desciption elements in the lines\n",
    "        element of the output dictionary.\n",
    "        \n",
    "    returns\n",
    "    -------\n",
    "    \n",
    "    Returns the output dictionary with the po_line_number element filled in.\n",
    "    \n",
    "    \"\"\"\n",
    "    is_po = po_lines['po_number'] == output['header']['po_number']\n",
    "    filtered_pos = po_lines[is_po]\n",
    "    filtered_pos = filtered_pos[['stock_code', 'stock_description', 'line_number']]\n",
    "    filtered_pos_list = filtered_pos.to_dict(orient='records')\n",
    "    filtered_pos = filtered_pos.set_index('stock_code')\n",
    "    filtered_pos_dict = filtered_pos.to_dict(orient='index')\n",
    "    matches = []\n",
    "    for i, line in enumerate(output['lines']):\n",
    "        best_match_line_number = 0\n",
    "        best_match_result = 0\n",
    "        if filtered_pos_dict.get(line['po_item'], '') != '': # remove +'y' for production\n",
    "            output['lines'][i]['po_line_number'] = filtered_pos_dict[line['po_item']]['line_number']\n",
    "        else:\n",
    "            print(line['description'])\n",
    "            for j, po_line in enumerate(filtered_pos_list):\n",
    "                match_result = fuzz.partial_ratio(line['description'], po_line['stock_description'])\n",
    "                if match_result > best_match_result and match_result not in matches:\n",
    "                    best_match_result = match_result\n",
    "                    best_match_line_number = j+1\n",
    "                print(' --', match_result, po_line['stock_description'])\n",
    "            print(best_match_result, best_match_line_number)\n",
    "            print()\n",
    "            output['lines'][i]['po_line_number'] = best_match_line_number\n",
    "            matches.append(best_match_line_number)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build HTML file for handling rejections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_html_section(data, html_template):\n",
    "    template = Template(html_template)\n",
    "    html = template.render(data=data)\n",
    "    return html\n",
    "\n",
    "\n",
    "reject_queue_html_template = \"\"\"\n",
    "    <div class=\"column is-full\">\n",
    "        <p><strong>Total number of rejects in queue: {{data.reject_count}}</strong></p>\n",
    "    </div>\n",
    "    <div class=\"column is-full\">  \n",
    "        <h2><strong>{{data.output.header.supplier}}</strong></h2>\n",
    "        <p><strong>Invoice number:</strong> {{data.output.header.invoice_number}}</p>\n",
    "    </div>\n",
    "    <div class=\"column is-two-fifths\">\n",
    "    <section class=\"section has-text-right\">\n",
    "        <p><strong>Net:</strong> {{\"$%.2f\"|format(data.output.header.net_total|float)}}</p>\n",
    "        <p><strong>GST:</strong> {{\"$%.2f\"|format(data.output.header.gst_total|float)}}</p>\n",
    "        <p><strong>Total:</strong> {{\"$%.2f\"|format(data.output.header.gross_total|float)}}</p>\n",
    "    </section>\n",
    "    <section class=\"section\">\n",
    "        <table class=\"table\">\n",
    "        <thead><tr><th>Field</th><th>Message</th></tr></thead>\n",
    "        <tbody>\n",
    "        {% for message in data.messages %}\n",
    "        <tr>\n",
    "            <th>{{message.field}}</th>\n",
    "            <td>{{message.message}}</td>\n",
    "        </tr>\n",
    "        {% endfor %}\n",
    "        </tbody>\n",
    "        </table>\n",
    "    </section>\n",
    "    </div>\n",
    "    <div class=\"column is-three-fifths has-text-centered\">\n",
    "        <p><a href=\"{{data.review_link}}\" target=\"_blank\">Review invoice</a></p>\n",
    "        <p><img src=\"{{data.invoice_image}}\" alt=\"Invoice Image\" width=\"250\"></p>\n",
    "    </div>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_reject_html(api_key, workflow_id, status, html_template):\n",
    "    queue = get_xtracta_status(api_key, workflow_id, status)\n",
    "    reject_count = len(queue)\n",
    "    document_id = queue[0][\"document_id\"]\n",
    "    reasons = queue[0][\"rejection\"][\"reason\"]\n",
    "    messages = get_reject_info(api_key, document_id, reasons)\n",
    "    document = get_document(api_key, document_id)\n",
    "    image_url = document[\"documents_response\"][\"document\"][\"image_url\"][0]\n",
    "    output = create_output(document)\n",
    "    review_link = open_document_ui(api_key, document_id)\n",
    "    data = {\n",
    "        \"output\": output,\n",
    "        \"reject_count\": reject_count,\n",
    "        \"review_link\": review_link,\n",
    "        \"invoice_image\": image_url,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "    html = create_html_section(data=data, html_template=html_template)\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reject_info(api_key, document_id, reasons):\n",
    "    messages = []\n",
    "    document = get_document(api_key, document_id)\n",
    "    field_ids = get_field_ids(document)\n",
    "    if type(reasons) != list:\n",
    "        field_id = reasons[\"linked_field\"][\"field_id\"]\n",
    "        message = reasons[\"message\"]\n",
    "        messages.append({\"field\": field_ids[field_id], \"message\": message})\n",
    "    else:\n",
    "        for sub_item in reasons:\n",
    "            field_id = sub_item[\"linked_field\"][\"field_id\"]\n",
    "            message = sub_item[\"message\"]\n",
    "            messages.append({\"field\": field_ids[field_id], \"message\": message})\n",
    "    return messages\n",
    "\n",
    "\n",
    "def get_field_ids(document):\n",
    "    field_ids = {}\n",
    "    fields = document[\"documents_response\"][\"document\"][\"field_data\"][\"field\"]\n",
    "    for field in fields:\n",
    "        field_ids[field[\"field_id\"]] = field[\"field_name\"]\n",
    "    return field_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build code\n",
    "\n",
    "The remaining cells load the code to PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "build"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook xtracta.ipynb to python\n",
      "[NbConvertApp] Writing 19320 bytes to xtracta.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pipomatic_hudge_xtracta.py'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!jupyter nbconvert \\\n",
    "    --TagRemovePreprocessor.enabled=True \\\n",
    "    --TagRemovePreprocessor.remove_cell_tags=\"['build', 'test']\" \\\n",
    "    --TemplateExporter.exclude_output=True \\\n",
    "    --to python \"xtracta.ipynb\"\n",
    "\n",
    "first_line = \"\"\"'Xtracta package'\n",
    "\n",
    "__version__ = '2.1'\n",
    "\n",
    "\"\"\"\n",
    "script_file = Path.cwd() / 'xtracta.py'\n",
    "script = script_file.read_text()\n",
    "script_file.write_text(first_line + script)\n",
    "username = script_file.parent.parent.name\n",
    "system_name = script_file.parent.name\n",
    "standardised_script_name = f'pipomatic_{username}_{system_name}.py'\n",
    "script_file.replace(script_file.parent / standardised_script_name)\n",
    "standardised_script_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "build"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reformatted pipomatic_hudge_xtracta.py\n",
      "All done! \\u2728 \\U0001f370 \\u2728\n",
      "1 file reformatted.\n"
     ]
    }
   ],
   "source": [
    "!black \"pipomatic_hudge_xtracta.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "build"
    ]
   },
   "outputs": [],
   "source": [
    "# !flit publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
