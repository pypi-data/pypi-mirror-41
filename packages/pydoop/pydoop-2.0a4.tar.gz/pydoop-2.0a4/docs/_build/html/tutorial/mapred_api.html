
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Writing Full-Featured Applications &#8212; Pydoop 2.0a4 documentation</title>
    <link rel="stylesheet" href="../_static/sphinxdoc.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Installation" href="../installation.html" />
    <link rel="prev" title="The HDFS API" href="hdfs_api.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../installation.html" title="Installation"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="hdfs_api.html" title="The HDFS API"
             accesskey="P">previous</a> |</li>
	<li><a href="../index.html">Home</a>|&nbsp;</li>
	<li><a href="../installation.html">Download & Install</a>|&nbsp;</li>
	<li><a href="https://github.com/crs4/pydoop/issues">Support</a>|&nbsp;</li>
	<li><a href="https://github.com/crs4/pydoop">Git Repo</a>|&nbsp;</li>
	<li><a href="https://crs4.github.io/pydoop/_pydoop1">Pydoop 1</a></li>

          <li class="nav-item nav-item-1"><a href="index.html" accesskey="U">Tutorial</a> &#187;</li> 
      </ul>
    </div>

      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/logo.png" alt="Logo"/>
            </a></p>
            <h3><a href="../index.html">Table Of Contents</a></h3>
            <ul>
<li><a class="reference internal" href="#">Writing Full-Featured Applications</a><ul>
<li><a class="reference internal" href="#mappers-and-reducers">Mappers and Reducers</a></li>
<li><a class="reference internal" href="#counters-and-status-updates">Counters and Status Updates</a></li>
<li><a class="reference internal" href="#record-readers-and-writers">Record Readers and Writers</a></li>
<li><a class="reference internal" href="#partitioners-and-combiners">Partitioners and Combiners</a></li>
<li><a class="reference internal" href="#profiling-your-application">Profiling Your Application</a></li>
</ul>
</li>
</ul>

            <h4>Previous topic</h4>
            <p class="topless"><a href="hdfs_api.html"
                                  title="previous chapter">The HDFS API</a></p>
            <h4>Next topic</h4>
            <p class="topless"><a href="../installation.html"
                                  title="next chapter">Installation</a></p>

					<h4>Get Pydoop</h4>
					<ul>
						<li> <a href="https://pypi.python.org/pypi/pydoop">Download page</a> </li>
						<li> <a href="../installation.html"> Installation Instructions </a> </li>
					</ul>

					<h4>Contributors</h4>
					<p class="topless">
					Pydoop is developed by:
					<a href="http://www.crs4.it">
						<img src="../_static/crs4.png" alt="CRS4" width="200" height="60" />
					</a>
					</p>
          <div id="searchbox" style="display: none">
            <h3>Quick search</h3>
              <form class="search" action="../search.html" method="get">
                <input type="text" name="q" size="18" />
                <input type="submit" value="Go" />
                <input type="hidden" name="check_keywords" value="yes" />
                <input type="hidden" name="area" value="default" />
              </form>
              <p class="searchtip" style="font-size: 90%">
              Enter search terms or a module, class or function name.
              </p>
          </div>
          <script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>


    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="writing-full-featured-applications">
<span id="api-tutorial"></span><h1>Writing Full-Featured Applications<a class="headerlink" href="#writing-full-featured-applications" title="Permalink to this headline">¶</a></h1>
<p>While <a class="reference internal" href="pydoop_script.html#pydoop-script-tutorial"><span class="std std-ref">Pydoop Script</span></a> allows to solve
many problems with minimal programming effort, some tasks require a
broader set of features.  If your data is not structured into simple
text lines, for instance, you may need to write a record reader; if
you need to change the way intermediate keys are assigned to reducers,
you have to write your own partitioner.  These components are
accessible via the Pydoop MapReduce API.</p>
<p>The rest of this section serves as an introduction to MapReduce
programming with Pydoop; the <a class="reference internal" href="../api_docs/mr_api.html#mr-api"><span class="std std-ref">API reference</span></a> has
all the details.</p>
<div class="section" id="mappers-and-reducers">
<h2>Mappers and Reducers<a class="headerlink" href="#mappers-and-reducers" title="Permalink to this headline">¶</a></h2>
<p>The Pydoop API is object-oriented: the application developer writes a
<a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.Mapper" title="pydoop.mapreduce.api.Mapper"><code class="xref py py-class docutils literal notranslate"><span class="pre">Mapper</span></code></a> class, whose core job is
performed by the <a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.Mapper.map" title="pydoop.mapreduce.api.Mapper.map"><code class="xref py py-meth docutils literal notranslate"><span class="pre">map()</span></code></a> method, and
a <a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.Reducer" title="pydoop.mapreduce.api.Reducer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Reducer</span></code></a> class that processes data via
the <a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.Reducer.reduce" title="pydoop.mapreduce.api.Reducer.reduce"><code class="xref py py-meth docutils literal notranslate"><span class="pre">reduce()</span></code></a> method.  The
following snippet shows how to write the mapper and reducer for the
<a class="reference internal" href="pydoop_script.html#word-count"><span class="std std-ref">word count</span></a> problem:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pydoop.mapreduce.api</span> <span class="kn">as</span> <span class="nn">api</span>
<span class="kn">import</span> <span class="nn">pydoop.mapreduce.pipes</span> <span class="kn">as</span> <span class="nn">pipes</span>


<span class="k">class</span> <span class="nc">Mapper</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">Mapper</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>
            <span class="n">context</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Reducer</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">Reducer</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">context</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>


<span class="n">FACTORY</span> <span class="o">=</span> <span class="n">pipes</span><span class="o">.</span><span class="n">Factory</span><span class="p">(</span><span class="n">Mapper</span><span class="p">,</span> <span class="n">reducer_class</span><span class="o">=</span><span class="n">Reducer</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">pipes</span><span class="o">.</span><span class="n">run_task</span><span class="p">(</span><span class="n">FACTORY</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>The mapper is instantiated by the MapReduce framework that, for each
input record, calls the <code class="docutils literal notranslate"><span class="pre">map</span></code> method passing a <code class="docutils literal notranslate"><span class="pre">context</span></code> object to it.
The context serves as a communication interface between the framework
and the application: in the map method, it is used to get the current
key (not used in the above example) and value, and to emit (send back
to the framework) intermediate key-value pairs.  The reducer works in
a similar way, the main difference being the fact that the <code class="docutils literal notranslate"><span class="pre">reduce</span></code>
method gets a set of values for each key.  The context has several
other functions that we will explore later.</p>
<p>To run the above program, save it to a <code class="docutils literal notranslate"><span class="pre">wc.py</span></code> file and execute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pydoop</span> <span class="n">submit</span> <span class="o">--</span><span class="n">upload</span><span class="o">-</span><span class="n">file</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">cache</span> <span class="n">wc</span><span class="o">.</span><span class="n">py</span> <span class="n">wc</span> <span class="nb">input</span> <span class="n">output</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">input</span></code> is the HDFS input directory.</p>
<p>See the section on <a class="reference internal" href="../running_pydoop_applications.html#running-apps"><span class="std std-ref">running Pydoop programs</span></a> for
more details.  Source code for the word count example is located under
<code class="docutils literal notranslate"><span class="pre">examples/wordcount</span></code> in the Pydoop distribution.</p>
</div>
<div class="section" id="counters-and-status-updates">
<h2>Counters and Status Updates<a class="headerlink" href="#counters-and-status-updates" title="Permalink to this headline">¶</a></h2>
<p>Hadoop features application-wide counters that can be set and
incremented by developers.  Status updates are arbitrary text messages
sent to the framework: these are especially useful in cases where the
computation associated with a single input record can take a
considerable amount of time, since Hadoop kills tasks that read no
input, write no output and do not update the status within a
configurable amount of time (ten minutes by default).</p>
<p>The following snippet shows how to modify the above example to use
counters and status updates:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Mapper</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">Mapper</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mapper</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="n">context</span><span class="o">.</span><span class="n">set_status</span><span class="p">(</span><span class="s2">&quot;initializing mapper&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_words</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_counter</span><span class="p">(</span><span class="s2">&quot;WORDCOUNT&quot;</span><span class="p">,</span> <span class="s2">&quot;INPUT_WORDS&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="n">context</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">context</span><span class="o">.</span><span class="n">increment_counter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">input_words</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Reducer</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">Reducer</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Reducer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="n">context</span><span class="o">.</span><span class="n">set_status</span><span class="p">(</span><span class="s2">&quot;initializing reducer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_words</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_counter</span><span class="p">(</span><span class="s2">&quot;WORDCOUNT&quot;</span><span class="p">,</span> <span class="s2">&quot;OUTPUT_WORDS&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reduce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="n">context</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">key</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">context</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
        <span class="n">context</span><span class="o">.</span><span class="n">increment_counter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_words</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Counter values and status updates show up in Hadoop’s web interface.
In addition, the final values of all counters are listed in the
command line output of the job (note that the list also includes Hadoop’s
default counters).</p>
</div>
<div class="section" id="record-readers-and-writers">
<h2>Record Readers and Writers<a class="headerlink" href="#record-readers-and-writers" title="Permalink to this headline">¶</a></h2>
<p>By default, Hadoop assumes you want to process plain text and splits
input data into text lines.  If you need to process binary data, or
your text data is structured into records that span multiple lines,
you need to write your own <a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.RecordReader" title="pydoop.mapreduce.api.RecordReader"><code class="xref py py-class docutils literal notranslate"><span class="pre">RecordReader</span></code></a>.
The <strong>record reader</strong> operates at the HDFS file level: its job is to read
data from the file and feed it as a stream of key-value pairs
(records) to the Mapper. To interact with HDFS files, we need to import the <code class="docutils literal notranslate"><span class="pre">hdfs</span></code> submodule:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pydoop.hdfs</span> <span class="kn">as</span> <span class="nn">hdfs</span>
</pre></div>
</div>
<p>The following example shows how to write a record reader that mimics
Hadoop’s default <code class="docutils literal notranslate"><span class="pre">LineRecordReader</span></code>, where keys are byte offsets
with respect to the whole file and values are text lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Reader</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">RecordReader</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Mimics Hadoop&#39;s default LineRecordReader (keys are byte offsets with</span>
<span class="sd">    respect to the whole file; values are text lines).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Reader</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">LOGGER</span><span class="o">.</span><span class="n">getChild</span><span class="p">(</span><span class="s2">&quot;Reader&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;started&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">isplit</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">input_split</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="s2">&quot;filename&quot;</span><span class="p">,</span> <span class="s2">&quot;offset&quot;</span><span class="p">,</span> <span class="s2">&quot;length&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span>
                <span class="s2">&quot;isplit.{} = {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">isplit</span><span class="p">,</span> <span class="n">a</span><span class="p">))</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">isplit</span><span class="o">.</span><span class="n">filename</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">isplit</span><span class="o">.</span><span class="n">offset</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bytes_read</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">isplit</span><span class="o">.</span><span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">discarded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bytes_read</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">discarded</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;closing open handles&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">next</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bytes_read</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">isplit</span><span class="o">.</span><span class="n">length</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>
        <span class="n">key</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">isplit</span><span class="o">.</span><span class="n">offset</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bytes_read</span>
        <span class="n">record</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">readline</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">record</span><span class="p">:</span>  <span class="c1"># end of file</span>
            <span class="k">raise</span> <span class="ne">StopIteration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bytes_read</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">record</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">get_progress</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">min</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bytes_read</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">isplit</span><span class="o">.</span><span class="n">length</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p>From the context, the record reader gets the following information on
the byte chunk assigned to the current task, or <strong>input split</strong>:</p>
<ul class="simple">
<li>the name of the file it belongs to;</li>
<li>its offset with respect to the beginning of the file;</li>
<li>its length.</li>
</ul>
<p>This allows to open the file, seek to the correct offset and read
until the end of the split is reached.  The framework gets the record
stream by means of repeated calls to the
<a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.RecordReader.next" title="pydoop.mapreduce.api.RecordReader.next"><code class="xref py py-meth docutils literal notranslate"><span class="pre">next()</span></code></a> method.  The
<a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.RecordReader.get_progress" title="pydoop.mapreduce.api.RecordReader.get_progress"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_progress()</span></code></a> method is
called by the framework to get the fraction of the input split that’s
already been processed.  The <code class="docutils literal notranslate"><span class="pre">close</span></code> method (present in all
components except for the partitioner) is called by the framework once
it has finished retrieving the records: this is the right place to
perform cleanup tasks such as closing open handles.</p>
<p>To use the reader, pass the class object to the factory with
<code class="docutils literal notranslate"><span class="pre">record_reader_class=Reader</span></code> and, when running the program with
<code class="docutils literal notranslate"><span class="pre">pydoop</span> <span class="pre">submit</span></code>, set the <code class="docutils literal notranslate"><span class="pre">--do-not-use-java-record-reader</span></code> flag.</p>
<p>The <strong>record writer</strong> writes key/value pairs to output files. The default
behavior is to write one tab-separated key/value pair per line; if you
want to do something different, you have to write a custom
<a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.RecordWriter" title="pydoop.mapreduce.api.RecordWriter"><code class="xref py py-class docutils literal notranslate"><span class="pre">RecordWriter</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Writer</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">RecordWriter</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Writer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">LOGGER</span><span class="o">.</span><span class="n">getChild</span><span class="p">(</span><span class="s2">&quot;Writer&quot;</span><span class="p">)</span>
        <span class="n">jc</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">job_conf</span>
        <span class="n">outfn</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">get_default_work_file</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;writing to </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">outfn</span><span class="p">)</span>
        <span class="n">hdfs_user</span> <span class="o">=</span> <span class="n">jc</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pydoop.hdfs.user&quot;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="n">hdfs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">outfn</span><span class="p">,</span> <span class="s2">&quot;wt&quot;</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="n">hdfs_user</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sep</span> <span class="o">=</span> <span class="n">jc</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;mapreduce.output.textoutputformat.separator&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;closing open handles&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">fs</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">emit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">key</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">sep</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The above example, which simply reproduces the default behavior, also
shows how to get job configuration parameters: the one starting with
<code class="docutils literal notranslate"><span class="pre">mapreduce</span></code> is a standard Hadoop parameter, while <code class="docutils literal notranslate"><span class="pre">pydoop.hdfs.user</span></code>
is a custom parameter defined by the application developer.
Configuration properties are passed as <code class="docutils literal notranslate"><span class="pre">-D</span> <span class="pre">&lt;key&gt;=&lt;value&gt;</span></code> (e.g.,
<code class="docutils literal notranslate"><span class="pre">-D</span> <span class="pre">mapreduce.output.textoutputformat.separator='|'</span></code>) to the submitter.</p>
<p>To use the writer, pass the class object to the factory with
<code class="docutils literal notranslate"><span class="pre">record_writer_class=Writer</span></code> and, when running the program with
<code class="docutils literal notranslate"><span class="pre">pydoop</span> <span class="pre">submit</span></code>, set the <code class="docutils literal notranslate"><span class="pre">--do-not-use-java-record-writer</span></code> flag.</p>
</div>
<div class="section" id="partitioners-and-combiners">
<h2>Partitioners and Combiners<a class="headerlink" href="#partitioners-and-combiners" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.Partitioner" title="pydoop.mapreduce.api.Partitioner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Partitioner</span></code></a> assigns intermediate keys to
reducers. By default, Hadoop uses <a class="reference external" href="https://hadoop.apache.org/docs/r3.0.0/api/org/apache/hadoop/mapreduce/lib/partition/HashPartitioner.html">HashPartitioner</a>,
which selects the reducer on the basis of a hash function of the key. If the
pipes factory does <em>not</em> provide a partitioner, partitioning will be done on
the Java side (by default, with <code class="docutils literal notranslate"><span class="pre">HashPartitioner</span></code>).</p>
<p>To write a custom partitioner in Python, subclass
<a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.Partitioner" title="pydoop.mapreduce.api.Partitioner"><code class="xref py py-class docutils literal notranslate"><span class="pre">Partitioner</span></code></a>, overriding the
<a class="reference internal" href="../api_docs/mr_api.html#pydoop.mapreduce.api.Partitioner.partition" title="pydoop.mapreduce.api.Partitioner.partition"><code class="xref py py-meth docutils literal notranslate"><span class="pre">partition()</span></code></a> method. The framework will
call this method with the current key and the total number of reducers <code class="docutils literal notranslate"><span class="pre">N</span></code>
as the arguments, and expect the chosen reducer ID — in the <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">...,</span>
<span class="pre">N-1]</span></code> range — as the return value.</p>
<p>The following examples shows how to write a partitioner that simply mimics the
default <code class="docutils literal notranslate"><span class="pre">HashPartitioner</span></code> behavior:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">hashlib</span> <span class="kn">import</span> <span class="n">md5</span>
<span class="k">class</span> <span class="nc">Partitioner</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">Partitioner</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Partitioner</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">LOGGER</span><span class="o">.</span><span class="n">getChild</span><span class="p">(</span><span class="s2">&quot;Partitioner&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">partition</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">n_reduces</span><span class="p">):</span>
        <span class="n">reducer_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">md5</span><span class="p">(</span><span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">(),</span> <span class="mi">16</span><span class="p">)</span> <span class="o">%</span> <span class="n">n_reduces</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;reducer_id: </span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">reducer_id</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">reducer_id</span>
</pre></div>
</div>
<p>The combiner is functionally identical to a reducer, but it is run
locally, on the key-value stream output by a single mapper.  Although
nothing prevents the combiner from processing values differently from
the reducer, the former, provided that the reduce function is
associative and idempotent, is typically configured to be the same as
the latter, in order to perform local aggregation and thus help cut
down network traffic.</p>
<p>Local aggregation is implemented by caching intermediate key/value pairs in a
dictionary. Like in standard Java Hadoop, cache size is controlled by
<code class="docutils literal notranslate"><span class="pre">&quot;mapreduce.task.io.sort.mb&quot;</span></code> and defaults to 100 MB. Pydoop uses
<a class="reference external" href="https://docs.python.org/2.7/library/sys.html#sys.getsizeof" title="(in Python v2.7)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sys.getsizeof()</span></code></a> to determine key/value size, which takes into account
Python object overhead. This can be quite substantial (e.g.,
<code class="docutils literal notranslate"><span class="pre">sys.getsizeof(b&quot;foo&quot;)</span> <span class="pre">==</span> <span class="pre">36</span></code>) and must be taken into account if fine tuning
is desired.</p>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p>Due to the caching, when using a combiner there are additional
limitations on the types that can be used for intermediate keys and
values. First of all, keys must be <a class="reference external" href="https://docs.python.org/3/glossary.html">hashable</a>. In addition, if a mutable type
is used for values, then values should not change after they have been
emitted by the mapper. For instance, the following (however contrived)
example would not work as expected:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">intermediate_value</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">class</span> <span class="nc">Mapper</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">Mapper</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">):</span>
     <span class="n">intermediate_value</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
     <span class="n">intermediate_value</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">value</span>
     <span class="n">ctx</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="n">intermediate_value</span><span class="p">)</span>
</pre></div>
</div>
<p class="last">For these reasons, it is recommended to use immutable types for both keys
and values when the job includes a combiner.</p>
</div>
<p>Custom partitioner and combiner classes must be declared to the factory as
done above for record readers and writers. To recap, if we need to use all of
the above components, we need to instantiate the factory as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">FACTORY</span> <span class="o">=</span> <span class="n">pipes</span><span class="o">.</span><span class="n">Factory</span><span class="p">(</span>
    <span class="n">Mapper</span><span class="p">,</span>
    <span class="n">reducer_class</span><span class="o">=</span><span class="n">Reducer</span><span class="p">,</span>
    <span class="n">record_reader_class</span><span class="o">=</span><span class="n">Reader</span><span class="p">,</span>
    <span class="n">record_writer_class</span><span class="o">=</span><span class="n">Writer</span><span class="p">,</span>
    <span class="n">partitioner_class</span><span class="o">=</span><span class="n">Partitioner</span><span class="p">,</span>
    <span class="n">combiner_class</span><span class="o">=</span><span class="n">Reducer</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="profiling-your-application">
<h2>Profiling Your Application<a class="headerlink" href="#profiling-your-application" title="Permalink to this headline">¶</a></h2>
<p>Python has built-in support for application <a class="reference external" href="https://docs.python.org/3/library/profile.html">profiling</a>. Profiling a standalone
program is relatively straightforward: run it through <code class="docutils literal notranslate"><span class="pre">cProfile</span></code>, store
stats in a file and use <code class="docutils literal notranslate"><span class="pre">pstats</span></code> to read and interpret them. A MapReduce
job, however, spawns multiple map and reduce tasks, so we need a way to
collect all stats. Pydoop supports this via a <code class="docutils literal notranslate"><span class="pre">pstats_dir</span></code> argument to
<code class="docutils literal notranslate"><span class="pre">run_task</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pipes</span><span class="o">.</span><span class="n">run_task</span><span class="p">(</span><span class="n">factory</span><span class="p">,</span> <span class="n">pstats_dir</span><span class="o">=</span><span class="s2">&quot;pstats&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>With the above call, Pydoop will run each MapReduce task with <code class="docutils literal notranslate"><span class="pre">cProfile</span></code>,
and store resulting pstats files in the <code class="docutils literal notranslate"><span class="pre">&quot;pstats&quot;</span></code> directory on HDFS. If
you’re using <code class="docutils literal notranslate"><span class="pre">pydoop</span> <span class="pre">submit</span></code>, you can also enable profiling via the
<code class="docutils literal notranslate"><span class="pre">--pstats-dir</span></code> command line argument:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pydoop submit --pstats-dir HDFS_DIR <span class="o">[</span>...<span class="o">]</span>
</pre></div>
</div>
<p>If the pstats directory is specified both ways, the one from <code class="docutils literal notranslate"><span class="pre">run_task</span></code>
takes precedence.</p>
<p>Another way to do time measurements is via counters. The <code class="docutils literal notranslate"><span class="pre">utils.misc</span></code> module
provides a <code class="docutils literal notranslate"><span class="pre">Timer</span></code> object for this purpose:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydoop.utils.misc</span> <span class="kn">import</span> <span class="n">Timer</span>

<span class="k">class</span> <span class="nc">Mapper</span><span class="p">(</span><span class="n">api</span><span class="o">.</span><span class="n">Mapper</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Mapper</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span> <span class="o">=</span> <span class="n">Timer</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">time_block</span><span class="p">(</span><span class="s2">&quot;tokenize&quot;</span><span class="p">):</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">:</span>
            <span class="n">context</span><span class="o">.</span><span class="n">emit</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>With the above coding, the total time spent to execute
<code class="docutils literal notranslate"><span class="pre">context.value.split()</span></code> (in ms) will be automatically accumulated in
a <code class="docutils literal notranslate"><span class="pre">TIME_TOKENIZE</span></code> counter under the <code class="docutils literal notranslate"><span class="pre">Timer</span></code> counter group.</p>
<p>Since profiling and timers can substantially slow down the Hadoop job, they
should only be used for performance debugging.</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="../installation.html" title="Installation"
             >next</a> |</li>
        <li class="right" >
          <a href="hdfs_api.html" title="The HDFS API"
             >previous</a> |</li>
	<li><a href="../index.html">Home</a>|&nbsp;</li>
	<li><a href="../installation.html">Download & Install</a>|&nbsp;</li>
	<li><a href="https://github.com/crs4/pydoop/issues">Support</a>|&nbsp;</li>
	<li><a href="https://github.com/crs4/pydoop">Git Repo</a>|&nbsp;</li>
	<li><a href="https://crs4.github.io/pydoop/_pydoop1">Pydoop 1</a></li>

          <li class="nav-item nav-item-1"><a href="index.html" >Tutorial</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2009-2019, CRS4.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.8.4.
    </div>
  </body>
</html>